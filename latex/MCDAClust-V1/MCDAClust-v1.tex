%\documentclass{iosart2c}
\documentclass[]{elsarticle}

\usepackage{natbib}
\usepackage{amsmath,amsthm,mathtools}
\usepackage{amssymb}
\usepackage{mathtools, cuted}

\usepackage[table, dvipsnames]{xcolor}
\usepackage{verbatim}

\usepackage{mathrsfs}
\usepackage{stfloats}
\usepackage{tabularx}
\usepackage{xtab,booktabs}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{tikz,ifthen,calc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.11}
\usepackage{tkz-fct}
\usepackage{color,colortbl}
\usepackage[labelfont=bf]{caption}


\usetikzlibrary{graphdrawing}
%\usetikzlibrary{graphicx}
%\usegdlibrary{trees}
\usetikzlibrary{shapes,decorations,arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows.meta}


\usepackage{enumitem}
\usepackage{algorithmic}
\usepackage{subcaption,graphicx}
%\usepackage{subfig,graphicx}
\usepackage[titletoc,title]{appendix}

%\usepackage[blocks]{authblk}
%\newenvironment{keywords}{\noindent\textbf{Keywords:}}{}

\theoremstyle{definition}
\newtheorem*{inner}{\innerheader}
\newcommand{\innerheader}{}
\newenvironment{defi}[1]
 {\renewcommand\innerheader{#1}\begin{inner}}
 {\end{inner}}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\newtheorem{prop}{Theorem}


\newcolumntype{d}[1]{D{.}{.}{#1}}



\journal{}
\bibliographystyle{model2-names}\biboptions{authoryear}

\begin{document}
\begin{frontmatter}                           % The preamble begins here.


\title{Stochastic clustering in the Human Development Index problem based on the outranking preference relation and $K$-means}

%\runningtitle{Hesitant Fuzzy Sets and ELECTRE TRI-C}
%\subtitle{Subtitle}


%\author[A]{\fnms{Javier} \snm{Pereira}\thanks{Corresponding author. E-mail: jpereirar@inacap.cl.}},
%\author[B]{\fnms{Elaine C.B.} \snm{de Oliveira}}
%\author[C]{\fnms{Danielle C.} \snm{Morais}}
%\author[C]{\fnms{Ana Paula C.S.} \snm{Costa}}
%and
%\author[C]{\fnms{Luciana H.} \snm{Alencar}}
%
%\runningauthor{F. Author et al.}
%\address[A]{Universidad Tecnol\'ogica de Chile INACAP,  Santiago, Chile\\
%E-mail: xjavierpereira7@gmail.com}
%\address[B]{Instituto Federal da Paraiba, Campus Jo\~ao Pessoa, Jo\~ao Pessoa, Brazil\\
%E-mail: elainecjz@gmail.com}
%\address[C]{Universidade Federal de Pernambuco, Recife, Brazil\\
%E-mail: \{dcmorais,apcabral,lhalencar\}@cdsid.org.br}

\author[utc]{Javier Pereira$^{*,}$\cortext[cor1]}
\author[ap]{Pedro Contreras}
\author[ufpe]{Danielle C. Morais}
\author[itesm]{Pilar Arroyo-L\'opez}



\cortext[cor1]{Corresponding author at: xjavierpereira7@gmail.com (J.Pereira)}

\address[utc]{Universidad Tecnol\'ogica de Chile Inacap, Santiago, Chile (xjavierpereira7@gmail.com); \\}
\address[ap]{Berlin, Germany (pedro.contreras@gmail.com);}
\address[ufpe]{Universidade Federal de Pernambuco, CDSID (dcmorais@cdsid.org.br);}
\address[itesm]{Tecnologico de Monterrey, Campus Toluca, M\'exico (pilar.arroyo@itesm.mx);}



\begin{abstract}
The Human Development Index (HDI) has been proposed as a mean to encourage nations to focus on people capabilities to develop a country.  HDI calculation is based on three indices: the life expectancy, the years of education and the Gross National Income. Several studies criticize the highly compensatory nature of the computation method and seeming precision of the information sources. Last decade, non-compensatory clustering approaches that consider imprecise information, integrating the $K$-means algorithm,  have been proposed as an alternative to the ranking process underlying the HDI calculation. However, in these methods two main drawbacks can be highlighted. Firstly, uncertainty regarding parameters used to model imprecision is not considered. Secondly,  as a consequence, robust ordered clusters have not been considered yet. In this paper, a process integrating  the $K$-means algorithm is presented in which uncertain parameters are considered, also finding  a robust ordered clustering.  This approach is applied and results are discussed and compared with the 2018 HDI ranking.
\end{abstract}

\begin{keyword}
HDI \sep clustering \sep $K$-means \sep robustness \sep outranking relation 
\end{keyword}

\end{frontmatter}


\section{Introduction}

since 1990, the United Nations Development Programme (UNDP) releases the Human Development Index (HDI) to emphasize that people and their capabilities should be the ultimate criteria for assessing the development of a country, not only economic growth. The HDI ranking process evaluates  179 United Nations countries on the basis of three criteria: the life expectancy, the years of education and the Gross National Income (GNI). In addition, based on the HDI quartiles, four ordered groups of countries can be identified: very high human development, high human development, medium human development, and low human development.  

Despite the broad use of HDI  in business, policy-making, research, development-political debates, allocation of development aid, international climate accord designs, and economics \citep{Wolff2011}, it has been object of three main criticisms \citep{Noorbakhsh1998, Berenger2007, Klugman2011, Martinez2013}: (1) the compensatory effect; (2) the underlying concepts to define the dimensions; (3) the categorization process that uses the COP quartiles.  Regarding the compensatory effect, it is easy to see that a country exhibiting a bad performance in one index, can compensate it with a good performance in another. However, this is not desirable since, for instance, bad health indices can be hidden by good GNI, which can exacerbes inequality in access to health systems.   

Non-compensatory  methods have been proposed as an alternative to the HDI calculation method. For instance, \cite{Natoli2011} analyze three aggregation techniques designed for benchmarking and ranking of countries according to aggregated dimensions: additive methods, geometric aggregations, and non-compensatory methods.  They focus on the Condorcet approach as a non-compensatory aggregation technique for progress measures.  \cite{Lozano2009} propose to use the minimum of the component indexes in HDI instead of the arithmetic average. \cite{Mazziotta2015} compare two non-compensatory composite indices for measuring multidimensional phenomena and monitoring their changes over time: a non-linear composite index and a two-parameter function, which is an intermediate case between a compensatory and a full non-compensatory index. The authors found that the non-linear composite seems to be less compensatory that the second one.

Several scholars have proposed  ordered clustering strategies to partition the set of countries included in the HDI ranking,  based on non-compensatory multi-criteria approaches \citep{DeSmet2014}.   \cite{Boujelben2016} propose an approach that integrates PROMETHEE and $K$-means, a popular greedy algorithm for partitioning a set of elements into a pre-defined number of clusters so as to minimize the distances to the cluster centroids (the cluster centers). The underlying idea of their approach is based on the notion of belief distance between the alternatives and the centroids. \cite{Chen2018} propose a PROMETHEE-based algorithm, which also integrates the $K$-means rationale, to cluster countries in the HDI Report. They found that the ordered clustering is highly consistent with the HDI ranks. \cite{DeSmet2012} propose an exact multi-criteria algorithm to find HDI ordered categories, based on valued preference degrees.  They show the consistency between the obtained ordered partition and the HDI ranking.  \cite{Monteiro2018} propose a supervised classification approach based on the ELECTRE TRI method in which the HDI categories are defined \emph{a priori} by using fixed category boundaries. Different from other proposals using the outranking-based models, these authors consider that there is no imprecise data in HDI information sources. 

However, two types of imprecision have been highlighted by authors analyzing the quality of information used in the HDI calculation.  Firstly, the weighting process assigning equal importance levels to dimensions in HDI has been criticized and methods to deal with have been proposed \citep{Zheng2015}. In this article, we are not considering this kind of imprecision.  Secondly, the importance of considering imprecise data in constructing composite indicators has been remarked \citep{Cherchye2011}.  There are multiple indicators used for different kind of indexes, for instance, the Environmental Performance Index, the Internal Market Index, the Human Development Index, or the Technology Achievement Index. However, for some indicators only interval information is available, within what a true value is believed to lie.   \cite{Wolff2011} analyzed three sources of data error in HDI calculation and classification: measurement error due to data revisions, data error due to formula updating and misclassification due to inconsistent categories cut-off values. Actually,  measurement error from data updating impacts the rough data, the indices normalization and the HDI calculation. They found that from 11\% up to 34\% of all countries could be interpreted as misclassified in the development categories. These authors also found that on average the expected absolute deviation is nine rank positions. These results mean that interpretation of HDI ranks must be very careful.  Moreover, methods applied to HDI, which are based on the outranking relation, as for instance those using the PROMETHEE or the ELECTRE TRI methods, consider stable parameters.  However, this choice seems to be inadequate since the imprecision introduced by data updating and   inconsistent categories cut-off values.  Thus, parameter values could be supposed to be uncertain, something that has not been yet considered in the HDI clustering literature.

Essentially, clustering processes integrating $K$-means lie in two features \citep{DeSmet2009,Lolli2014,Panapakidis2018, Chen2018}. First, the definition of  an indifference-based metric that measures how indifferent is an alternative to a central alternative, or centroid, representing a category. Thus, an alternative is assigned to the cluster where the most indifferent centroid is found. Second, once all the alternatives have been assigned, the centroids are updated, using the information regarding the alternatives in their respective groups.  The process continues until a stop condition is satisfied. 

Although the $K$-means and the outranking-based methods can be used to find the same number of country clusters proposed by the ranking-based HDI methodology, these use compensatory net flow aggregation processes to obtain global preference relations. Thus, the methods do not necessarily guarantee the separability between any pair of central actions representing two different categories, a desired property of preference-based classification processes \citep{roy2012}. This means that a strict preference relation between any pair of centroids at each criterion level is not necessarily satisfied. As a consequence, supplementary procedures must be defined to order the clusters.  In these methods,  an alternative must be compared to each other, at least once. This information is stored in the form of a $n \times n$ matrix which must be traversed through on each iteration of the clustering algorithm. These are $O(IKn^2) \sim O(n^2)$ processes, where $I, K$ are the number of iterations of the algorithm (before finding a solution) and the number of clusters, respectively. We argue that a $O(Kn)$ method may be implemented for the assignment process by simply comparing each country at most to the $K$ centroids in the model.  Actually, we propose the usage of the ELECTRE TRI-C sorting method rationale to assign alternatives, in which central reference alternatives are used to define the categories, but updating these centroids by applying the $K$-means approach.

In this article, a stochastic approach for ordered clustering in the HDI problem is proposed, which integrates an outranking-based classification process with $K$-means, such that:  (1) building ordered clusters is an integral part of the procedure, without need for supplementary processes; an assignment rule proposed in the ELECTRE TRI-C sorting method \citep{almeida2008,Almeida2012} is used in the process, extending rules proposed in sorting methods to clustering; (2) uncertain thresholds are considered such that robustness analysis is performed. Thus, the proposed approach is integrated  to a Monte Carlo Simulation process in which sets of  parameter values are passed as input to the ordered clustering algorithm. To the best of our knowledge, this is the first approach that deals with these two problems.

The article is organized as follows......

%\section{Human Development Index}
%
%Since the first publication in 1990, the Human Development Index has been used as a benchmark to encourage development of nations and to foster policy makers to focus on three dimensions:  long and healthy life, access to knowledge and a decent standard of living.  The HDI is the geometric mean of normalized indices for each of the three dimensions, constructed from four indicators \citep{UNDP2019}: Life expectancy at birth (LE); Expected years of schooling (EYS); Mean years of schooling (MYS); and Gross National Income per capita (GNIpc) .  
%
%In order to calculate the HDI, two steps are applied: (1) Creating the dimension indices; (2) Aggregating the dimension indices into the HDI.  Step (1) consists of normalizing the four mentioned indicators according to the following formula:
%
%\begin{equation}
%\mbox{Dimension index}=\frac{\mbox{indicator value - minimum value}}{\mbox{maximum value - minimum value}}. \label{index}
%\end{equation}
%
%\noindent
%In the case of the two normalized education indices, the arithmetic mean is taken.  For the GNI, the natural logarithm of the indicator, the minimum and the maximum values is taken. Next, in Step (2), the geometric mean of indices is taken, as follows:
%
%\begin{equation}
%HDI= (I_{Health} \cdot I_{Education} \cdot I_{Income})^{1/3}. \label{HDI}
%\end{equation}
%
%\noindent
%It is worth to note that in this formula the same weights for all dimensions are given.   Based on the 2014 HDI, four categories of human development achievements are defined, using fixed cutoff points (COP) calculated using the quartiles ($q$) from the distributions of the component indicators averaged over 2004–2013:
%
%\begin{equation}
%COP_q= HDI(LE_q,EYS_q,MYS_q,GNIpc_q), \, q=1,2,3. \label{COP}
%\end{equation}
%
%\noindent
%These quartiles are used in the 2019's Report, as follows:
%
%
%\begin{table}[hbtp]
%\centering
%\begin{tabular}{ll}
%Category						&Limits \\
%\hline
%Very high human development 		& 0.800 and above \\
%\hline
%High human development			& 0.700-0.799		\\
%\hline
%Medium human development		& 0.550-0.699		\\
%\hline
%Low human development			& Below 0.550\\
%\hline
%\end{tabular}
%\end{table}
%
%
%%\subsection{HDI clustering problem}
%
%Three main criticisms to HDI have been reported by different authors \citep{Noorbakhsh1998, Berenger2007, Klugman2011, Martinez2013}: (1) the compensatory effect; (2) the underlying concepts to define the dimensions; (3) the categorization process that uses the COP quartiles.  Regarding the compensatory effect, it is easy to see that a country exhibiting a bad performance in one index, can compensate it with a good performance in another. However, this is not desirable since, for instance, bad health indices can be hidden by good GNI, which can exacerbes inequality in access to health systems.   
%
%Non-compensatory  methods have been proposed as an alternative to the HDI calculation method. For instance, \cite{Natoli2011} analyze three aggregation techniques designed for benchmarking and ranking of countries according to aggregated dimensions: additive methods, geometric aggregations, and non-compensatory methods.  They focus on the Condorcet approach as a non-compensatory aggregation technique for progress measures.  \cite{Lozano2009} propose to use the minimum of the component indexes in HDI instead of the arithmetic average. \cite{Mazziotta2015} compare two non-compensatory composite indices for measuring multidimensional phenomena and monitoring their changes over time: a non-linear composite index and a two-parameter function, which is an intermediate case between a compensatory and a full non-compensatory index. The authors found that the non-linear composite seems to be less compensatory that the second one.
%
%
%Non-compensatory  methods have been proposed to classify countries in the HDI Report.   \cite{Boujelben2016} propose an approach that integrates PROMETHEE and $K$-means. The underlying idea of their approach is based on the notion of belief distance between the alternatives and the centroids. \cite{Chen2018} propose a PROMETHEE-based algorithm, which integrates the $K$-means rationale, to cluster countries in the HDI Report. They found that the ordered clustering is highly consistent with the HDI ranks. \cite{DeSmet2012} propose an exact multi-criteria algorithm to find HDI ordered categories, based on valued preference degrees.  They show the consistency between the obtained ordered partition and the HDI ranking.  \cite{Monteiro2018} propose a supervised classification approach based on the ELECTRE TRI method in which the HDI categories are defined \emph{a priori} by using fixed category boundaries. Different from other proposals using the outranking-based models, these authors consider that $p=q=0$, that is, there is no imprecision. 
%
%Two types of imprecision have been highlighted by authors analyzing the quality of information used in the HDI calculation.  Firstly, the weighting process assigning equal importance levels to dimensions in HDI has been criticized and methods to deal with have been proposed \citep{Zheng2015}. In this article, we are not considering this kind of imprecision.  Secondly, the importance of considering imprecise data in constructing composite indicators has been remarked \citep{Cherchye2011}.  There are multiple indicators used for different kind of indexes, for instance, the Environmental Performance Index, the Internal Market Index, the Human Development Index, or the Technology Achievement Index. However, for some indicators only interval information is available, within what a true value is believed to lie.   \cite{Wolff2011} analyzed three sources of data error in HDI calculation and classification: measurement error due to data revisions, data error due to formula updating and misclassification due to inconsistent categories cut-off values. Actually,  measurement error from data updating impacts the rough data, the indices normalization and the HDI calculation. They found that from 11\% up to 34\% of all countries could be interpreted as misclassified in the development categories. These authors also found that on average the expected absolute deviation is nine rank positions. These results mean that interpretation of HDI ranks must be very careful.   
%
%
%
%Thus, several scholars have proposed  ordered clustering strategies to find a partition of countries, which we call the \emph{HDI problem}, based on non-compensatory multi-criteria approaches \citep{DeSmet2014,Boujelben2016}.  Most of non-compensatory approaches for computing the HDI clustering use the global outranking relation to model imprecise information regarding the evaluation of alternatives. Outranking was early proposed in Elimination Et Choix Traidusant la Realite (ELECTRE) methods \citep{figueira2010} and further became an essential feature in  Preference Ranking Organization METHod for Enrichment of Evaluations (PROMETHEE) methods \cite{brans85}. The outranking relation helps to model global  assertions from kind ``the alternative $b$ is at least as good as the alternative $a$'', based on information collected at the level of each criteria. 
%
%Some of the new MCDA clustering approaches to HDI integrate the $K$-means rationale \citep{DeSmet2009,Lolli2014,Panapakidis2018, Chen2018}, a popular greedy algorithm for partitioning a set of elements into a pre-defined number of clusters so as to minimize the distances to the cluster centroids (the cluster centers). Essentially, these clustering processes lie in two features of $K$-means. First, the definition of  an indifference-based metric measures how indifferent is an alternative to a centroid. Thus, an alternative is assigned to the cluster where the most indifferent centroid is found. Second, once all the alternatives have been assigned, the centroids are updated, using the information regarding the alternatives in their respective groups.  The process continues until a stop condition is satisfied. 
%
%Although the the $K$-means and outranking-based methods can be used to find the same number of country clusters proposed by the ranking-based HDI methodology, these use compensatory net flow aggregation processes to obtain global preference relations. Thus, the methods do not necessarily guarantee the separability between any pair of central actions representing two different categories, a desired property of preference-based classification processes \citep{roy2012}. This means that a strict preference relation between any pair of centroids at each criterion level is not necessarily satisfied. As a consequence, supplementary procedures must be defined to order the clusters.  In these methods,  an alternative must be compared to each other, at least once. This information is stored in the form of a $n \times n$ matrix which must be traversed through on each iteration of the clustering algorithm. These are $O(IKn^2) \sim O(n^2)$ processes, where $I, K$ are the number of iterations of the algorithm (before finding a solution) and the number of clusters, respectively. We argue that a $O(Kn)$ method may be implemented for the assignment process by simply comparing each country at most to the $K$ centroids in the model.  
%
%Methods based in the outranking relation need to model imprecise information by using two kind of parameters, defined for each criterion \citep{figueira2010}: the indifference and the preference thresholds. Most of time, these parameters are defined  with the decision-maker (DM) or expert judgement assistance. However, in the case of information regarding the HDI analysis, there is not an identified DM or even expert judgement can be variable because sources of information to evaluate countries in the three criteria is uncertain.  Thus, in this problem, considering single values does not seem appropriate.   
%
%
%In this article, we are going to model imprecision by the indifference and preference thresholds. However, differently from other outranking-based approaches, the parameter values are supposed to be realizations of  stochastic variables. Thus, the proposed approach is integrated  to a Monte Carlo Simulation process in which sets of  parameter values are passed as input to the ordered clustering algorithm.  In which follows, this procedure is explained in detail.



\section{Notation}\label{notation}

\subsection{Outranking relation}\label{classification}

Most of non-compensatory approaches for computing the HDI clustering use the global outranking relation to model imprecise information regarding the evaluation of alternatives. Outranking was early proposed in Elimination Et Choix Traidusant la Realite (ELECTRE) methods \citep{figueira2010} and further became an essential feature in  Preference Ranking Organization METHod for Enrichment of Evaluations (PROMETHEE) methods \cite{brans85}. The outranking relation helps to model global  assertions from kind ``the alternative $b$ is at least as good as the alternative $a$'', based on information collected at the level of each criteria. 

Non-compensatory methods based on the outranking relation could lead to higher computational complexity. However, problems exist in which compensation among criteria is not allowable. We place the present article in that kind of situations and propose a clustering model based on the outranking relation, as defined in the ELECTRE family of methods \citep{figueira2010}.

Let $A=\{a_1,a_2,\ldots,a_n\}$ be a set of actions (alternatives);  let $F=\{g_1,g_2,\ldots,g_m\}$ be a coherent family of criteria used to evaluate each action and $W=\{w_j \mid 0 \leq w_j \leq 1, \sum_{j=1}^m w_j =1\}$ the criteria weights.  The vector of performances of an action  $(g_1(a),g_2(a),\ldots,g_m(a))$   is called the \emph{evaluation profile}. 



The outranking relation, interpreted as the assertion ``$b$ is at least as good as $a$'',  can be measured on each criterion in terms of  the \emph{partial direct concordance index}, defined as follows: 

{\footnotesize
\begin{equation}
c_j(b,a) =
\begin{cases}
	0							& \mbox{if  $g_j(a)-g_j(b) > p_j$}, \\
	1							& \mbox{if  $g_j(a)-g_j(b) \leq q_j$},   \\
	\dfrac{g_j(b)-g_j(a)+p_j}{p_j-q_j} 	& \mbox{otherwise}, 				     	      
 \end{cases} 
 \label{credibility}
 \end{equation} 
}

\noindent
where $p_j,q_j$ are two imprecision parameters called direct preference threshold and  direct indifference threshold, respectively.  Similarly,  let us consider the assertion ``$a$ is at least as good as $b$'', by using inverse thresholds $p'_j,q'_j$, such that a \emph{partial inverse concordance index} may be defined as 

{\fontsize{7}{7}
\begin{equation}
c^{inv}_j(a,b) =
\begin{cases}
	0							& \mbox{if  $g_j(b)-g_j(a) > p'_j$}, \\
	1							& \mbox{if  $g_j(b)-g_j(a) \leq q'_j$},   \\
	\dfrac{g_j(a)-g_j(b)+p'_j}{p'_j-q'_j} 	& \mbox{otherwise}. 				     	      
 \end{cases} 
 \label{invcredibility}
 \end{equation} 
}


\noindent
A detailed explanation regarding the properties that direct and inverse thresholds must satisfy can be found in \cite{roy2012}.  The partial direct and inverse concordance indices allow to compute the global concordance indices as follows:

\begin{eqnarray}
\sigma_D(b_h,a) 	&=& \sum_{j=1}^{m} w_j c_j(b_h,a),\label{generalsigmaDG}\\
\sigma_I(a,b_h) 	&=& \sum_{j=1}^{m} w_j c^{inv}_j(a,b_h). \label{generalsigmaIG}
\end{eqnarray} 


\subsection{Outranking-based classification}\label{outrankingclass}

Let $C_h, (h=1,\ldots,H; H\geq 2)$ be a set of ordered categories such that $C_H \succ C_{H-1}\succ \ldots \succ C_1$, where  the relation $\succ$ means that for any $C_i, C_ j $ such that $i>j$, an element $a_i \in C_i$ is not worse than any element $a_j \in C_j$. Conversely, $a_j$ is worse than $a_i$, in terms of preferences.   A special set of actions $B=\{b_1,\ldots, b_H\}$ is defined, in which $b_h$ represents the \emph{centroid} of the category $C_h$. 



\def \q {1}
\def \p {2}
\def \qq {1.5}
\def \pp {3}
\def \xmina{-3}
\def \xmaxa{.5}
\def \xminb{-.5}
\def \xmaxb{4}
\def \ymax{1}
\def \xb{2*\p}

\begin{figure}[hbtp]
\scriptsize
\centering
\begin{tikzpicture}
\begin{axis}[
		clip=false,
		xmin=0,xmax=5,
		ymin=0,ymax=1,
		height=4cm,
		width=7cm,
		axis line style={draw=none}, 
		xticklabels=\empty,	
		xtick=\empty,
		xtick={0},	
		ytick=\empty,
		xtick pos=left,
		xtick distance=1,
		every axis x label/.style={at={(current axis.right of origin)},anchor=west}
		]
		\addplot [domain=-\p:-\q,smooth, thick] {(x+\p)/(\p-\q)};
		\addplot [domain=\xmina:-\p,smooth, thick] {0};
		\addplot [domain=-\q:\qq,smooth, thick] {1};
		\addplot [smooth,dashed]  coordinates {(-\q,0)(-\q,\ymax)} node{} ;
		\addplot [smooth,->]  coordinates {(\xmina,0)(\xmina,\ymax+.5)} node[above]{$\mu_j(a,b_h)$} ;
		\addplot [domain=\pp:\xmaxb,smooth, thick] {0};
		\addplot [domain=-\p:\xmaxb,smooth,thin,->] {0};
		\addplot [domain=\qq:\pp,smooth, thick] {(-x+\pp)/(\pp-\qq)};
		\addplot [smooth,dashed]  coordinates {(\qq,0)(\qq,\ymax)} node{} ;
    		\node  at (axis cs:\qq-.1,-0.1) {\tiny$10+q_j$};
    		\node  at (axis cs:\pp,-0.1) {\tiny$10+p_j$};
    		\node  at (axis cs:-\p-.3,-0.1) {\tiny$10-p_j'$};
    		\node  at (axis cs:-\q-.1,-0.1) {\tiny$10-q_j'$};
    		\node  at (axis cs:0,-0.1) {\tiny$10$};
		\node  at (axis cs:4.5,0) {${\tiny g_j(a)}$};
\end{axis}
\end{tikzpicture}
\caption{Membership function in continuous and discrete cases}
\label{mu}
\end{figure}


Let us define the following trapezoidal fuzzy number (TrFN)  \citep{Ban2011}:

\begin{equation}
\mu_j(a,b_h)=\min\{c_j(b_h,a), c^{inv}_j(a,b_h)\},
\label{trfnmu}
\end{equation}


\noindent
which can be interpreted as a fuzzy indifference  relation, constructed from two outranking relations \citep{perny1992}. In Figure \ref{mu},  an example is shown where $b_h$, with $g_j(b_h)=10$ on a criterion $j$,  is the centroid of a class $C_h$. Thus, any alternative $a$ can be evaluated as belonging to that class or not, by using the membership function $\mu_j(a,b_h)$.

Conversely, deconstructing $\mu_j(a,b_h)$ into the two outranking relations $c_j(b_h,a)$ and  $c^{inv}_j(a,b_h)$, the credibility indices \eqref{generalsigmaDG} and \eqref{generalsigmaIG} may be calculated and the following assignment rules can be defined \citep{pereira2018}:

\begin{enumerate}
\item
\emph{Descending Rule}. Let $\lambda \in [0.5,1]$ be a minimum credibility level. Decrease $h$ from $H+1$ until the first $t$ such that $\sigma_I(a,b_t)\geq \lambda$:

\begin{enumerate}
\item
If $t=H+1$, assign $a$ to $C_H$.
\item
If $t=0$, assign $a$ to $C_1$.
\item
For $0<t<H+1$, 

if $\min\{\sigma_I(a,b_t),\sigma_D(b_t,a)\} > \min\{\sigma_I(a,b_{t+1}),\sigma_D(b_{t+1},a)\}$ then assign $a$ to $C_t$; otherwise, assign $a$ to $C_{t+1}$.
\end{enumerate} 

\item
\emph{Ascending Rule}. Let $\lambda \in [0.5,1]$ be a minimum credibility level. Increase $h$ from $0$ until the first $t$ such that $\sigma_D(b_t,a)\geq \lambda$:

\begin{enumerate}
\item
If $t=1$, assign $a$ to $C_1$.
\item
If $t=H+1$, assign $a$ to $C_H$.
\item
For $0<t<H+1$, 

if $\min\{\sigma_I(a,b_t),\sigma_D(b_t,a)\} > \min\{\sigma_I(a,b_{t-1}),\sigma_D(b_{t-1},a)\}$ then assign $a$ to $C_t$; otherwise, assign $a$ to $C_{t-1}$.
\end{enumerate} 
\end{enumerate}


Essentially, the descending and the ascending rules evaluate how indifferent an alternative $a$ is to every $b_h$.  Thus, $a$ is assigned to the class for which it is closest.  In practice, these rules may assign an alternative to different classes. Therefore, in a particular application of the algorithm presented below, just one of them must be chosen.


\subsection{$K$-means algorithm}

$K$-means is a classical unsupervised method to partitioning a set of elements into a set of disjoint homogeneous groups, by minimizing the following objective function 

\begin{equation}
\min \sum_{h=1}^{K} \sum_{i=1}^{n} I_{ih} \left\| a_i-b_h \right\|,
\end{equation}


\noindent
where  $I_{ih}=1$ if $a_i \in C_h$, $0$ otherwise; and $ \left\| a_i-b_h \right\|$ is the Euclidean distance between $a_i$ and $b_h$.  Classical $K$-means algorithm consists of three steps: (1) define $A, K, b_h$ as inputs; (2) assign elements of $A$ into clusters; (3) update the cluster centers and return to step (2), until a stop condition is satisfied.   

Step (2) assigns an element $a_i$ to the group where the Euclidean distance between the element and the respective cluster center is minimized.  Step (3) updates the cluster centroids as follows


\begin{equation}
g_j(b_h) = \frac{1}{\mid C_h \mid} \sum_{a_i \in C_h} g_j(a_i) \quad j=1,\ldots,m. \label{newcentroid}
\end{equation}

\noindent
Thus, the algorithm is iterated a number of times, or until the  change of centroids is less or equal than a given tolerance.


\section{Clustering approach}\label{methodology}

The approach proposed here assumes that $K\geq 2$ ordered clusters must be found: $C_K \succ \ldots \succ C_1$. Similarly to the original $K$-means algorithm, an iterative constructive clustering process is performed until a stop condition is satisfied, as described  in Figure \ref{clustering}.  

Initially, the set of alternatives, the family of criteria, the criteria weights, and the number of clusters are identified.  Next, the imprecision parameters are set. Stochastic preference and indifference thresholds are considered in this approach. Thus, each time that the Step 2 is performed, a new set of values for $p_j,q_j,p'_j,q'_j \,(j=1,\ldots,m)$ are generated and used as input to the Step 3, where the new initial centroids are defined. Centroids  must satisfy the strict separability condition \citep{roy2012}:  for any pair $b_h^{(0)}, b_k^{(0)}$ such that $h>k$, it follows 

\begin{equation}
b_h^{(0)} \,P\, b_k^{(0)} \Leftrightarrow g_j(b_h^{(0)})-p'_j \geq g_j(b_k^{(0)})+p_j, \forall j.
\end{equation}

\noindent
where $P$ is a binary relation representing a strong preference.  This condition makes that fuzzy sets representing the membership functions of every category do not overlap. 


Step 4 consists of computing the credibility indices, using the expressions \eqref{generalsigmaDG} and  \eqref{generalsigmaIG}. In Step 5, the alternatives are assigned into the classes, using the descending (or the ascending) rule proposed in Section \ref{outrankingclass}.

In Step 6, the evaluation profile of each new centroid in the iteration $t$ and class $C_h$ is computed as follows:

\begin{equation}
g_j(b_h^{(t)}) = \frac{1}{\mid C_h^{t,j} \mid} \sum_{a_i \in C_h^{t,j}} g_j(a_i) \quad j=1,\ldots,m,\label{newcentroid}
\end{equation}

\noindent
where $C_h^{t,j}=\{a_i \in C_h \mid g_j(b_h^{(t)})-p'_j\leq g_j(a_i) \leq g_j(b_h^{(t)})+p_j\}$.  Whenever $C_h^{t,j}=\emptyset$, then $g_j(b_h^{(t)})=g_j(b_h^{(t-1)})$.   This condition holds the separability condition, as shown in the following theorem.

\begin{prop}
If  $g_j(b_h^{(t)})$ is defined by using \eqref{newcentroid}, then $b_h^{(t)} \,P\, b_k^{(t)}, h>k$.
\end{prop}

\noindent

The proof of this theorem is provided in the \ref{sepcondition}.




\begin{figure}[hbtp]
\begin{center}
\fontsize{7}{7}{
\tikzstyle{bigbox}=[align=center,rectangle, draw=black, rounded corners,  anchor=north, minimum width=3.5cm, text width=3cm, minimum height=.7cm, node distance=.4cm]
\tikzstyle{myarrow}=[->, >= triangle 45]
\begin{tikzpicture}[
	scale=0.7,
	blueb/.style={
  		draw=black,
¡  		rounded corners,
  		text width=2cm,
  		font={\sffamily\color{black}}}
		]
    \node (defafw)[bigbox]
        {
            	1. Define $A, F, W, K, s=0$
        };
    \node (defpq)[bigbox,below= of defafw]
        {
            	2. Define $p_j,p'_j,q_j,q'_j,s=s+1$
        };
    \node (initcentroids)[bigbox,below= of defpq]
        {
            	3. Define $t=0, b_h^{(0)}$
        };
    \node (indices) [bigbox,   below=  of initcentroids]
        {
            	4. Compute $\sigma_D, \sigma_I$
	};
    \node (assign) [bigbox,   below=  of indices]
        {
            	5. Assign alternatives
	};
    \node (centroid) [bigbox,   below= of assign]
        {
            	6. Compute $t=t+1$ and  $b_h^{(t)}$ 
	};
   \node (decision) [draw, diamond, align=center, minimum width=2.5cm, minimum height=2.5cm, aspect=2, inner sep=-3pt, below=.4cm of centroid] 
        {
        $\mid b_h^{(t)} -b_h^{(t-1)} \mid < \epsilon$,\\
        or\\
	$t=Maxit$
        };
   
   \node (stochastic) [draw, diamond, align=center,  minimum width=3.0cm, minimum height=2.5cm, aspect=2, inner sep=-3pt, below=.4cm of decision] 
        {
	$s=Maxst$
        };

    \node (final) [bigbox,   below=of stochastic]
        {7. Compute frequency indices
	};
       
   \draw[myarrow] (defafw.south) -> (defpq.north);
   \draw[myarrow] (defpq.south) -> (initcentroids.north);
   \draw[myarrow] (initcentroids.south) -> (indices.north);
   \draw[myarrow] (indices.south) -> (assign.north);
   \draw[myarrow] (assign.south) -> (centroid.north);
  \draw[myarrow] (centroid.south) -> (decision.north);
  \draw[myarrow] (decision.south) node[left] {yes} -> (stochastic.north);
  \draw[myarrow] (decision.west) |- node[above] {no} ++(-17mm,0) --  ($(indices.west) + (-15mm,0)$) -> (indices.west);
  \draw[myarrow] (stochastic.south) node[left] {yes} -> (final.north);
  \draw[myarrow] (stochastic.west) |- node[above] {no} ++(-30mm,0) --  ($(defpq.west) + (-27mm,0)$) -> (defpq.west);

\end{tikzpicture}
}
\caption{Outranking-based approach to ordered clustering}
\label{clustering}   
\end{center}
\end{figure}

Given a set of parameter values, the inner loop in Figure \ref{clustering} iterates up to one out of two stop conditions applies: 1) the change of evaluation profiles is  lower than a pre-defined $\epsilon$ value, or 2) the maximum number of iterations, $Maxit$,  is reached.  The whole process is repeated $Maxst$ times, once for each set of stochastic parameter values. In Step 7, the final frequency indices are computed, proposing the $K$ ordered clusters.


  

In the following section, this approach is applied to the HDI problem.





\section{Application}\label{application}





\subsection{Running the process}



\subsection{Results}


\subsection{Discussion}



%A Brazilian electrical power company has 49 projects that need to be clustered such that scarce and specialized resources could be allocated in a right manner. In a recent work by \cite{Oliveira2016},  a family of criteria was defined for this problem. The following criteria are defined in this problem: ($g_1$) \emph{Project Complexity}, which increases according to the increasing of budget and number of departments involved in the project's development; ($g_2$) \emph{Resources}, measuring the number of man-hours required to complete the project; ($g_3$)  \emph{Expected rate of development}, evaluating the urgency in project development and implementation; ($g_4$) \emph{Contribution} to the achievement of the organizational strategy; ($g_5$)  \emph{Technological level} involved in the project development. In Table \ref{criteria}, scales and weights, defined by the three DMs in the process, are shown.  
%
%
%\begin{table}[hbtp]
%\caption{Criteria, scales and weights}
%\label{criteria}
%\scriptsize
%\begin{center}
%\begin{tabular}{clllll}
%\hline
%						&\multicolumn{3}{c}{Weight}							&					&			\\\cline{2-4}
%Criterion					&DM1			&DM2			&DM3			&{\bf Verbal scale} 		&{\bf Scale} 	\\\hline
%\multirow{4}{*}{$g_1$}		& \multirow{4}{*}{0.20}& \multirow{4}{*}{0.15}& \multirow{4}{*}{0.30}&High				&4	 		\\
%						& 				&				&				&Medium				&3	 		\\
%						& 				&				&				&Low				&2	 		\\
%						& 				&				&				&Very low				&1	 		\\
%						& 				&				&				&					&	 		\\
%$g_2$					&0.20 				&0.15			&0.25			&					&Man-hours	 \\
%						& 				&				&				&					&	 		\\
%\multirow{5}{*}{$g_3$}		& \multirow{5}{*}{0.20}	& \multirow{5}{*}{0.20}	& \multirow{5}{*}{0.15}&Urgent				&5	 		\\
%						& 				&				&				&Critical				&4	 		\\
%						&				&				&				&Competitive			&3	 		\\
%						&				&				&				&Regular				&2	 		\\
%						&				&				&				&Low				&1	 		\\
%						&				&				&				&					&	 		\\
%\multirow{4}{*}{$g_4$}		& \multirow{4}{*}{0.30}& \multirow{4}{*}{0.15}& \multirow{4}{*}{0.15}	&Very high			&4	 		\\
%						& 				&				&				&High				&3	 		\\
%						& 				&				&				&Medium				&2	 		\\
%						& 				&				&				&Low				&1	 		\\
%						& 				&				&				&					&	 		\\
%\multirow{4}{*}{$g_5$}		& \multirow{4}{*}{0.10}& \multirow{4}{*}{0.35}& \multirow{4}{*}{0.15}	&High				&4	 		\\
%						& 				&				&				&Medium				&3	 		\\
%						& 				&				&				&Low				&2	 		\\
%						& 				&				&				&Negligible			&1	 		\\
%\hline
%
%\end{tabular}
%\end{center}
%\end{table}
%
%
%
%Evaluation of 49 projects are shown in Appendix  \ref{evaluations}. A senior manager helped to define reference alternatives, also shown in that table. According to  \cite{Oliveira2016}, three categories can be defined in this problem: $C_1$, the non-critical projects; $C_2$, the set of critical projects; $C_3$, the set of very critical projects.  


\section{Conclusions}\label{conclusions}
  


     
\section*{Acknowledgement}

%The authors are grateful for the support from FACEPE (PRONEX) and for the partial support of CNPq, Brazil.
          
\section*{References}
\bibliographystyle{authoryear}
\bibliography{../bib/mybib-mcdm,../bib/mybib-mcdm-v1,../bib/mybib}



%\newpage
%\appendices
%\section{Project evaluations}\label{evaluations}
%
%\begin{table}[hbtp]
%	\centering
%	\scriptsize
%	\begin{tabular}{rllrrrrr}
%Project	&	$g_1$	&	$g_2$	&	$g_3$	&	$g_4$	&	$g_5$	\\\hline
%1	&	2	&	640	&	2	&	3	&	2	\\
%2	&	3	&	480	&	2	&	2	&	1	\\
%3	&	3	&	640	&	1	&	3	&	2	\\
%4	&	1	&	720	&	2	&	2	&	2	\\
%5	&	1	&	160	&	3	&	3	&	2	\\
%6	&	2	&	160	&	2	&	3	&	2	\\
%7	&	3	&	620	&	2	&	1	&	2	\\
%8	&	2	&	640	&	1	&	2	&	1	\\
%9	&	1	&	320	&	2	&	3	&	3	\\
%10	&	2	&	320	&	2	&	3	&	2	\\
%11	&	2	&	160	&	3	&	3	&	1	\\
%12	&	1	&	160	&	1	&	1	&	2	\\
%13	&	2	&	640	&	3	&	3	&	2	\\
%14	&	2	&	240	&	3	&	3	&	2	\\
%15	&	1	&	160	&	2	&	2	&	1	\\
%16	&	1	&	160	&	2	&	3	&	1	\\
%17	&	3	&	320	&	3	&	2	&	1	\\
%18	&	3	&	640	&	2	&	3	&	1	\\
%19	&	3	&	960	&	2	&	3	&	2	\\
%20	&	1	&	160	&	2	&	2	&	2	\\
%21	&	2	&	640	&	2	&	2	&	2	\\
%22	&	1	&	160	&	2	&	2	&	3	\\
%23	&	3	&	1280	&	1	&	2	&	2	\\
%24	&	1	&	320	&	3	&	2	&	2	\\
%25	&	2	&	160	&	1	&	2	&	2	\\
%26	&	1	&	160	&	2	&	2	&	1	\\
%27	&	2	&	1280	&	1	&	3	&	2	\\
%28	&	3	&	640	&	2	&	2	&	2	\\
%29	&	2	&	800	&	1	&	2	&	2	\\
%30	&	2	&	160	&	3	&	2	&	2	\\
%31	&	2	&	160	&	1	&	2	&	1	\\
%32	&	3	&	320	&	1	&	2	&	2	\\
%33	&	2	&	240	&	2	&	3	&	1	\\
%34	&	2	&	320	&	2	&	2	&	1	\\
%35	&	1	&	120	&	2	&	3	&	1	\\
%36	&	2	&	160	&	1	&	3	&	1	\\
%37	&	2	&	3200	&	1	&	2	&	1	\\
%38	&	2	&	3200	&	1	&	2	&	1	\\
%39	&	2	&	960	&	1	&	2	&	1	\\
%40	&	2	&	3840	&	1	&	2	&	1	\\
%41	&	2	&	2880	&	1	&	2	&	1	\\
%42	&	1	&	1200	&	3	&	2	&	1	\\
%43	&	1	&	1200	&	3	&	2	&	1	\\
%44	&	3	&	7680	&	4	&	3	&	3	\\
%45	&	1	&	2400	&	3	&	2	&	1	\\
%46	&	3	&	5760	&	4	&	3	&	1	\\
%47	&	3	&	2880	&	4	&	3	&	1	\\
%48	&	2	&	7680	&	3	&	3	&	1	\\
%49	&	1	&	7680	&	3	&	3	&	1	\\\hline
%$b_1$	&	1	&	300	&	2	&	1	&	1	\\
%$b_2$	&	2	&	2000	&	3	&	2	&	2	\\
%$b_3$	&	3	&	5000	&	4	&	3	&	3	\\\hline
%\end{tabular}
%\end{table}
%


\appendix
\section{Separability condition}\label{sepcondition}


\end{document}

