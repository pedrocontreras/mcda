%\documentclass{iosart2c}
\documentclass[]{elsarticle}

\usepackage{natbib}
\usepackage{amsmath,amsthm,mathtools}
\usepackage{amssymb}
\usepackage{mathtools, cuted}

\usepackage[table, dvipsnames]{xcolor}
\usepackage{verbatim}

\usepackage{mathrsfs}
\usepackage{stfloats}
\usepackage{tabularx}
\usepackage{xtab,booktabs}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{tikz,ifthen,calc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.11}
\usepackage{tkz-fct}
\usepackage{color,colortbl}
\usepackage[labelfont=bf]{caption}


\usetikzlibrary{graphdrawing}
%\usetikzlibrary{graphicx}
%\usegdlibrary{trees}
\usetikzlibrary{shapes,decorations,arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows.meta}


\usepackage{enumitem}
\usepackage{algorithmic}
\usepackage{subcaption,graphicx}
%\usepackage{subfig,graphicx}
\usepackage[titletoc,title]{appendix}

%\usepackage[blocks]{authblk}
%\newenvironment{keywords}{\noindent\textbf{Keywords:}}{}

\theoremstyle{definition}
\newtheorem*{inner}{\innerheader}
\newcommand{\innerheader}{}
\newenvironment{defi}[1]
 {\renewcommand\innerheader{#1}\begin{inner}}
 {\end{inner}}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\newtheorem{prop}{Proposition}


\newcolumntype{d}[1]{D{.}{.}{#1}}



\journal{}
\bibliographystyle{model2-names}\biboptions{authoryear}

\begin{document}
\begin{frontmatter}                           % The preamble begins here.


\title{Clustering in the Human Development Index problem based on the outranking preference relation and $K$-means}

%\runningtitle{Hesitant Fuzzy Sets and ELECTRE TRI-C}
%\subtitle{Subtitle}


%\author[A]{\fnms{Javier} \snm{Pereira}\thanks{Corresponding author. E-mail: jpereirar@inacap.cl.}},
%\author[B]{\fnms{Elaine C.B.} \snm{de Oliveira}}
%\author[C]{\fnms{Danielle C.} \snm{Morais}}
%\author[C]{\fnms{Ana Paula C.S.} \snm{Costa}}
%and
%\author[C]{\fnms{Luciana H.} \snm{Alencar}}
%
%\runningauthor{F. Author et al.}
%\address[A]{Universidad Tecnol\'ogica de Chile INACAP,  Santiago, Chile\\
%E-mail: xjavierpereira7@gmail.com}
%\address[B]{Instituto Federal da Paraiba, Campus Jo\~ao Pessoa, Jo\~ao Pessoa, Brazil\\
%E-mail: elainecjz@gmail.com}
%\address[C]{Universidade Federal de Pernambuco, Recife, Brazil\\
%E-mail: \{dcmorais,apcabral,lhalencar\}@cdsid.org.br}

\author[utc]{Javier Pereira$^{*,}$\cortext[cor1]}
\author[ap]{Pedro Contreras}
\author[ufpe]{Danielle C. Morais}
\author[itesm]{Pilar Arroyo-L\'opez}



\cortext[cor1]{Corresponding author at: xjavierpereira7@gmail.com (J.Pereira)}

\address[utc]{Universidad Tecnol\'ogica de Chile Inacap, Santiago, Chile (xjavierpereira7@gmail.com); \\}
\address[ap]{WMC, The University of Warwick (pedro.contreras@acm.org);}
\address[ufpe]{Universidade Federal de Pernambuco, CDSID (dcmorais@cdsid.org.br);}
\address[itesm]{Tecnologico de Monterrey, Campus Toluca, M\'exico (pilar.arroyo@itesm.mx);}



\begin{abstract}
The human development index (HDI) has been proposed as a mean to foster nations to put focus on people capabilities to develop a country.  HDI calculation bases on three criteria: the life expectancy, the years of education and the gross national income. Several studies criticize the highly compensatory nature and apparently precision of the computation method. Thus, clustering approaches that consider imprecise information  have been proposed as an alternative to the ranking process underlying the HDI computation. However, most of these methods are based on some kind of compensatory global aggregation process, and the assumption that precise information regarding the evaluation criteria is available.    In this paper, a non-compensatory ordered clustering process integrating  the $K$-means algorithm is presented. Differently from other approaches, parameters that help to model imprecision are considered un certain and robustness analysis is performed.  An indifference preference relation is used to group projects and to build a set of $K$ ordered clusters, satisfying a separability condition between pairs of centroids. Results are compared with outcomes obtained with PROMETHEE-based and other outranking-based approaches.
\end{abstract}

\begin{keyword}
HDI index \sep clustering \sep outranking relation \sep $K$-means
\end{keyword}

\end{frontmatter}


\section{Introduction}

The human development index (HDI) was created by the United Nations Development Programme (UNDP) to emphasize that people and their capabilities should be the ultimate criteria for assessing the development of a country, not only economic growth. The United Nations Development Program (UNDP) has proposed the HDI ranking which is used to evaluate  179 United Nations countries on the basis of three criteria: the life expectancy, the years of education and the Gross National Income (GNI). For each country, a ranking can be obtained by aggregating these three criteria.  HDI is computed as the geometric mean of normalized indices for each one of these criteria.  In 2018, the Human Development Report Office, responsible for the HDI, identified four ordered groups of countries, based on quartiles: very high human development, high human development, medium human development, and low human development.   

HDI has been object of criticism because of the highly compensatory computation model and the lack of consideration in which regards the precision of information available to evaluate the countries and the information available to model preferences \citep{Noorbakhsh1998, Berenger2007, Klugman2011, Martinez2013}.  Thus, several scholars have proposed  ordered clustering strategies to find a partition of countries, based on non-compensatory multi-criteria approaches \citep{DeSmet2014,Boujelben2016}.  A method is compensatory whether bad performance on some criteria can be offset by good performance on others.  Instead, non-compensatory methods use parameters and implement aggregation rules that allow to eliminate alternatives that perform very bad in one or several criteria \citep{figueira2010}.  

Most of non-compensatory approaches for analyzing HDI countries use the global outranking relation to model imprecise information regarding the evaluation of alternatives. Outranking was early proposed in Elimination Et Choix Traidusant la Realite (ELECTRE) methods \citep{figueira2010} and further became an essential feature in  Preference Ranking Organization METHod for Enrichment of Evaluations (PROMETHEE) methods \cite{brans85}. The outranking relation helps to model global  assertions from kind ``the alternative $b$ is at least as good as the alternative $a$'', based on information collected at the level of each criteria. 

HDI analysis has been proposed by integrating clustering techniques with PROMETHEE \citep{Chen2018} and classification with ELECTRE \citep{doCarvalhalMonteiro2018} approaches. Some of the new clustering approaches in MCDA integrate the $K$-means rationale \citep{DeSmet2009,Lolli2014,Panapakidis2018}, a popular greedy algorithm for partitioning a set of elements into a pre-defined number of clusters so as to minimize the distances to the cluster centroids (the cluster centers). Essentially, these clustering processes lie in two features of $K$-means. First, the definition of a distance metric to evaluate how similar are an alternative and a centroid.  In MCDA approaches,  instead, an indifference-based metric measures how indifferent is an alternative to a centroid. Thus, an alternative is assigned to the cluster where the most indifferent centroid is found. Second, once all the alternatives have been assigned, the centroids are updated, using the information regarding the alternatives in their respective groups.  The process continues until a stop condition is satisfied. 

Although the the $K$-means and outranking-based methods can be used to found the same number of country clusters proposed by the HDI methodology, these use compensatory net flow aggregation processes to obtain global preference relations. Thus, the methods do not necessarily guarantee the separability between any pair of central actions representing two different categories, a desired property of preference-based classification processes \citep{roy2012}. This means that a strict preference relation between any pair of centroids at each criterion level is not necessarily satisfied. As a consequence, supplementary procedures must be defined to order the clusters.  Another characteristic of these methods is that an alternative must be compared to each other, at least once. This information is stored in the form of a $n \times n$ matrix which must be traversed through on each iteration of the clustering algorithm. These are $O(IKn^2) \sim O(n^2)$ processes, where $I, K$ are the number of iterations of the algorithm (before finding a solution) and the number of clusters, respectively. We argue that a $O(Kn)$ method may be implemented for the assignment process by simply comparing each country at most to the $K$ centroids in the model.  

In this article, we take into account these two difficulties and propose a new approach for ordered clustering of HDI countries which integrates an outranking-based classification process with $K$-means, that does not use any net flow aggregation process, and such that:  (1) building ordered clusters is an integral part of the procedure without need for supplementary processes; (2)  an assignment rule proposed in the ELECTRE TRI-C sorting method \citep{almeida2008,Almeida2012} is used in the process, extending rules proposed in sorting methods to clustering; (3) the centroids are updated by a subset of the elements in the respective cluster, such that the new centroids preserve the separability condition.  

Although the method is sensitive to the initial number of clusters, the analysis of the HDI set of countries, by using the Nearest Neighbors algorithm, reveals that three or four categories is adequate, which has been already found in recent studies \citep{doCarvalhalMonteiro2018}.  Running the classical $K$-means algorithm, the four initial centroids are defined.  This information and parameter values needed by the outranking relation are used as input to our algorithm.  Results show that for some of the elements proposed in a group by the HDI segmentation, our algorithm suggests these need to be placed in other categories.  Robustness analysis regarding the parameter values is performed to confirm that outcome and to draw up a conclusion.  We compare results with those obtained by other proposals in the literature.

The article is organized as follows......

\section{Clustering in MCDA}

Most problems in data analysis regard the partition of a set of elements, often described by multiple dimensions, into a set of homogeneous groups. There are two elementary ways to group elements: supervised and unsupervised methods.  The first concerns methods in which the elements are assigned into pre-defined categories. In such cases, the problem consists of identifying the ``right'' category for each element. The unsupervised methods are used in cases in which  a best partitioning of the set of elements must be discovered, such that the some distance metric is minimized between a pair of elements in a group and maximized between a pair of elements in different groups.  Clustering has become an important issue in Multi-Criteria Decision Analysis (MCDA).  Most decision-making problems require the assignment of alternatives into groups such that differentiated treatments and strategies can be defined, which implementation depends on the characteristics exhibited by alternatives belonging to each group. 

Last past decades,  several authors have proposed new MCDA methods that  integrate  $K$-means. However, two main issues must be considered. First, in data analysis,  a distance metric is defined to compare any pair of elements. In MCDA, instead, preference relations must be used to compare pairs of alternatives, which is often modeled by using value functions or outranking relations \citep{roy96, figueira2010}. Thus, the main difference between the classification strategies in data analysis  and the MCDA classification processes lies in the way that dimensions used to represent the elements are conceived.  In data analysis,  dimensions are conceptualized as attributes that help to characterize the elements in order to explore or recognize data patterns. Thus, two elements belonging to the same group are said \emph{mutually similar}.  In MCDA,  dimensions are used to evaluate elements and any pair of elements can be compared in terms of preference relations by using these evaluations. Although some authors have proposed similarity measures to determine whether an alternative belong to a group or not, it has been proposed that is better to use indifference measures  \citep{Olteanu2013}. Thus, two elements belonging to the same group are said \emph{mutually indifferent}. Nevertheless, below we propose that reviewing the indifference relation between the alternatives and the centroids also allows to discover ordered groups.

Secondly, in MCDA two unsupervised strategies are recognized, the first one called the \emph{ordered clustering}, in which clusters are completely ordered, and  \emph{relational clustering} which purpose is finding more complex order relations between clusters \citep{zopounidis2002, DeSmet2009}. $K$-means implements an unsupervised strategy to discover un-ordered clusters. Thus, integrating this algorithm into an MCDA method supposes the definition of a strategy to order the clusters built by the algorithm process.  Compensatory methods have incorporated these conditions.  \cite{Lolli2014} integrate $K$-means with the AHP method and apply it to a multi-criteria inventory classification problem. \cite{Panapakidis2018} integrate TOPSIS and $K$-means to cluster user load profiles based on consumption data (for instance, in energy related fields), weather data, consumer preferences, occupancy behavior and others. This is also a compensatory strategy.

For which concerns non-compensatory approaches, \cite{Chen2018} propose a PROMETHEE II and $K$-means algorithm which considers the preference degree between any two alternatives. A relative net flow metric, based on the outranking relation of PROMETHEE, is proposed to measure the closeness of alternatives and centroids. An ordered clustering is found by defining a \emph{preference distance} metric based on the global net flow of any alternative in a cluster.  \cite{Fernandez2010} also propose a model based on the outranking relation to cluster alternatives, using a $K$-means strategy. They define an indifference outranking relation to evaluate closeness of alternatives and centroids. Given that this clustering process does not necessarily produces an ordered set of clusters, the authors propose a specific method to build that clustering structure.  \cite{DeSmet2012} address the problem of regrouping alternatives into completely ordered categories based on valued preference degrees. An exact algorithm is proposed to find the ordered partition. They propose a model that takes as input both the number of clusters, fixed \emph{a priori}, and a valued preference matrix which stores the global strict preference value between any pair of alternatives. To compute the strict preference value, an outranking-based function is formulated. The purpose in that article is to find an ordered partition so as to minimize the inconsistencies with the information contained in this matrix. The inconsistencies regard the wrong assignment of pairs of alternatives into clusters  such that this does not satisfy the alternatives preference structure stored in the matrix. 


First, it can be verified that, given two centroids from successive ordered clusters, these methods do not necessarily satisfy  a consistency property which requires a strict preference relation be satisfied between any pair of centroids at each criterion level.  This is a structural feature in the ELECTRE TRI sorting methods \citep{roy2012}, which guarantees the separability between any pair of central actions representing two different categories. We argue that paying attention to this property, a clustering process can be defined, by integrating a $K$-means-like mechanism to update the CRA as if these were centroids, but in a way that the clusters order is preserved through the clustering process. 

Second, the assignment based in the indifference relation considers that any  alternative in a cluster should not be preferable to another inside the same group.   However, by construction, the alternatives in the cluster should be indifferent to the current centroid and not necessarily indifferent each other.  In which follows, we propose an algorithm based on this idea.  We show that this is an efficient way to obtain ordered clusters.


\section{Notation}\label{notation}

\subsection{Outranking relation}\label{classification}

Non-compensatory methods based on the outranking relation could lead to higher computational complexity. However, problems exist in which compensation among criteria is not allowable. We place the present article in that kind of situations and propose a clustering model based on the outranking relation, as defined in the ELECTRE family of methods \citep{figueira2010}.

Let $A=\{a_1,a_2,\ldots,a_n\}$ be a set of actions (alternatives);  let $F=\{g_1,g_2,\ldots,g_m\}$ be a coherent family of criteria used to evaluate each action and $W=\{w_j \mid 0 \leq w_j \leq 1, \sum_{j=1}^m w_j =1\}$ the criteria weights.  The vector of performances of an action  $(g_1(a),g_2(a),\ldots,g_m(a))$   is called the \emph{evaluation profile}. 



The outranking relation, interpreted as the assertion ``$b$ is at least as good as $a$'',  can be measured on each criterion in terms of  the \emph{partial direct concordance index}, defined as follows: 

{\footnotesize
\begin{equation}
c_j(b,a) =
\begin{cases}
	0							& \mbox{if  $g_j(a)-g_j(b) > p_j$}, \\
	1							& \mbox{if  $g_j(a)-g_j(b) \leq q_j$},   \\
	\dfrac{g_j(b)-g_j(a)+p_j}{p_j-q_j} 	& \mbox{otherwise}, 				     	      
 \end{cases} 
 \label{credibility}
 \end{equation} 
}

\noindent
where $p_j,q_j$ are two imprecision parameters called direct preference threshold and  direct indifference threshold, respectively.  Similarly,  let us consider the assertion ``$a$ is at least as good as $b$'', by using inverse thresholds $p'_j,q'_j$, such that a \emph{partial inverse concordance index} may be defined as 

{\fontsize{7}{7}
\begin{equation}
c^{inv}_j(a,b) =
\begin{cases}
	0							& \mbox{if  $g_j(b)-g_j(a) > p'_j$}, \\
	1							& \mbox{if  $g_j(b)-g_j(a) \leq q'_j$},   \\
	\dfrac{g_j(a)-g_j(b)+p'_j}{p'_j-q'_j} 	& \mbox{otherwise}. 				     	      
 \end{cases} 
 \label{invcredibility}
 \end{equation} 
}


\noindent
A detailed explanation regarding the properties that direct and inverse thresholds must satisfy can be found in \cite{roy2012}.  The partial direct and inverse concordance indices allow to compute the global concordance indices as follows:

\begin{eqnarray}
\sigma_D(b_h,a) 	&=& \sum_{j=1}^{m} w_j c_j(b_h,a),\label{generalsigmaDG}\\
\sigma_I(a,b_h) 	&=& \sum_{j=1}^{m} w_j c^{inv}_j(a,b_h). \label{generalsigmaIG}
\end{eqnarray} 


\subsection{Outranking-based classification}\label{outrankingclass}

Let $C_h, (h=1,\ldots,H; H\geq 2)$ be a set of ordered categories such that $C_H \succ C_{H-1}\succ \ldots \succ C_1$, where  the relation $\succ$ means that for any $C_i, C_ j $ such that $i>j$, an element $a_i \in C_i$ is not worse than any element $a_j \in C_j$. Conversely, $a_j$ is worse than $a_i$, in terms of preferences.   A special set of actions $B=\{b_1,\ldots, b_H\}$ is defined, in which $b_h$ represents the \emph{centroid} of the category $C_h$. 



\def \q {1}
\def \p {2}
\def \qq {1.5}
\def \pp {3}
\def \xmina{-3}
\def \xmaxa{.5}
\def \xminb{-.5}
\def \xmaxb{4}
\def \ymax{1}
\def \xb{2*\p}

\begin{figure}[hbtp]
\scriptsize
\centering
\begin{tikzpicture}
\begin{axis}[
		clip=false,
		xmin=0,xmax=5,
		ymin=0,ymax=1,
		height=4cm,
		width=7cm,
		axis line style={draw=none}, 
		xticklabels=\empty,	
		xtick=\empty,
		xtick={0},	
		ytick=\empty,
		xtick pos=left,
		xtick distance=1,
		every axis x label/.style={at={(current axis.right of origin)},anchor=west}
		]
		\addplot [domain=-\p:-\q,smooth, thick] {(x+\p)/(\p-\q)};
		\addplot [domain=\xmina:-\p,smooth, thick] {0};
		\addplot [domain=-\q:\qq,smooth, thick] {1};
		\addplot [smooth,dashed]  coordinates {(-\q,0)(-\q,\ymax)} node{} ;
		\addplot [smooth,->]  coordinates {(\xmina,0)(\xmina,\ymax+.5)} node[above]{$\mu_j(a,b_h)$} ;
		\addplot [domain=\pp:\xmaxb,smooth, thick] {0};
		\addplot [domain=-\p:\xmaxb,smooth,thin,->] {0};
		\addplot [domain=\qq:\pp,smooth, thick] {(-x+\pp)/(\pp-\qq)};
		\addplot [smooth,dashed]  coordinates {(\qq,0)(\qq,\ymax)} node{} ;
    		\node  at (axis cs:\qq-.1,-0.1) {\tiny$10+q_j$};
    		\node  at (axis cs:\pp,-0.1) {\tiny$10+p_j$};
    		\node  at (axis cs:-\p-.3,-0.1) {\tiny$10-p_j'$};
    		\node  at (axis cs:-\q-.1,-0.1) {\tiny$10-q_j'$};
    		\node  at (axis cs:0,-0.1) {\tiny$10$};
		\node  at (axis cs:4.5,0) {${\tiny g_j(a)}$};
\end{axis}
\end{tikzpicture}
\caption{Membership function in continuous and discrete cases}
\label{mu}
\end{figure}


Let us define the following trapezoidal fuzzy number (TrFN)  \citep{Ban2011}:

\begin{equation}
\mu_j(a,b_h)=\min\{c_j(b_h,a), c^{inv}_j(a,b_h)\},
\label{trfnmu}
\end{equation}


\noindent
which can be interpreted as a fuzzy indifference  relation, constructed from two outranking relations \citep{perny1992}. In Figure \ref{mu},  an example is shown where $b_h$, with $g_j(b_h)=10$ on a criterion $j$,  is the centroid of a class $C_h$. Thus, any alternative $a$ can be evaluated as belonging to that class or not, by using the membership function $\mu_j(a,b_h)$.

Conversely, deconstructing $\mu_j(a,b_h)$ into the two outranking relations $c_j(b_h,a)$ and  $c^{inv}_j(a,b_h)$, the credibility indices \eqref{generalsigmaDG} and \eqref{generalsigmaIG} may be calculated and the following assignment rules can be defined \citep{pereira2018}:

\begin{enumerate}
\item
\emph{Descending Rule}. Let $\lambda \in [0.5,1]$ be a minimum credibility level. Decrease $h$ from $H+1$ until the first $t$ such that $\sigma_I(a,b_t)\geq \lambda$:

\begin{enumerate}
\item
If $t=H+1$, assign $a$ to $C_H$.
\item
If $t=0$, assign $a$ to $C_1$.
\item
For $0<t<H+1$, 

if $\min\{\sigma_I(a,b_t),\sigma_D(b_t,a)\} > \min\{\sigma_I(a,b_{t+1}),\sigma_D(b_{t+1},a)\}$ then assign $a$ to $C_t$; otherwise, assign $a$ to $C_{t+1}$.
\end{enumerate} 

\item
\emph{Ascending Rule}. Let $\lambda \in [0.5,1]$ be a minimum credibility level. Increase $h$ from $0$ until the first $t$ such that $\sigma_D(b_t,a)\geq \lambda$:

\begin{enumerate}
\item
If $t=1$, assign $a$ to $C_1$.
\item
If $t=H+1$, assign $a$ to $C_H$.
\item
For $0<t<H+1$, 

if $\min\{\sigma_I(a,b_t),\sigma_D(b_t,a)\} > \min\{\sigma_I(a,b_{t-1}),\sigma_D(b_{t-1},a)\}$ then assign $a$ to $C_t$; otherwise, assign $a$ to $C_{t-1}$.
\end{enumerate} 
\end{enumerate}


Essentially, the descending and the ascending rules evaluate how indifferent an alternative $a$ is to every $b_h$.  Thus, $a$ is assigned to the class for which it is closest.  In practice, these rules may assign an alternative to different classes. Therefore, in a particular application of the algorithm presented below, just one of them must be chosen.


\subsection{$K$-means algorithm}

Here a brief description of $K$-means.


\section{Clustering approach}\label{methodology}

The approach proposed here assumes that $K\geq 2$ ordered clusters must be found: $C_K \succ \ldots \succ C_1$. Similarly to the original $K$-means algorithm, iterations are performed until a stop condition is satisfied, as described  in Figure \ref{clustering}.  Initially, the set of alternatives and the family of criteria are identified.  Next, the preference and imprecision parameters are elicited.  The clustering algorithm is initialized in step 3, defining the number of clusters and  the initial centroids. Step 4 consists of computing the credibility indices, using the expressions \eqref{generalsigmaDG} and  \eqref{generalsigmaIG}. In step 5, the alternatives are assigned to the classes, using the descending (or the ascending) rule proposed in Section \ref{outrankingclass}.

{\bf In Step 6, the new centroids are computed}.   In this step, the evaluation profile of each new centroid in class $C_h$ is computed as follows:

\begin{equation}
g_j(b_h^{(t)}) = \frac{1}{\mid C_h \mid} \sum_{a_i \in C_h} g_j(a_i) \quad j=1,\ldots,m \label{newcentroid}
\end{equation}

One of two possible stop conditions must be selected: 1) the change of evaluation profiles is  lower than a pre-defined $\epsilon$ value, or 2) $n$ iterations have been performed.  In step 7, a final recommendation is drawn up, proposing the $K$ ordered clusters.



\begin{figure}[hbtp]
\begin{center}
\fontsize{7}{7}{
\tikzstyle{bigbox}=[align=center,rectangle, draw=black, rounded corners,  anchor=north, minimum width=3.5cm, text width=3cm, minimum height=.7cm, node distance=.4cm]
\tikzstyle{myarrow}=[->, >= triangle 45]
\begin{tikzpicture}[
	scale=0.7,
	blueb/.style={
  		draw=black,
¡  		rounded corners,
  		text width=2cm,
  		font={\sffamily\color{black}}}
		]
    \node (defaf)[bigbox]
        {
            	1. Define $A, F$
        };
    \node (defparameters)[bigbox,below= of defaf]
        {
            	2. Elicit $W, p_j, q_j, p'_j, q'_j$
        };
    \node (initcentroids)[bigbox,below= of defparameters]
        {
            	3. Define $K$, $t=0$, $b_h^{(0)} (h=1,\ldots,K)$
        };
    \node (indices) [bigbox,   below=  of initcentroids]
        {
            	4. Compute credibility indices $\sigma_D, \sigma_I$
	};
    \node (assign) [bigbox,   below=  of indices]
        {
            	5. Assign alternatives
	};
    \node (centroid) [bigbox,   below= of assign]
        {
            	6. Calculate $t=t+1$ and \\
	 centroids $b_h^{(t)}$
	};
   \node (decision) [draw, diamond, align=center, minimum height=2.5cm, aspect=2, inner sep=-3pt, below=.4cm of centroid] 
        {
        $\mid b_h^{(t)} -b_h^{(t-1)} \mid < \epsilon$,\\
        or\\
	$it=n$
        };
   
    \node (final) [bigbox,   below=of decision]
        {
            	7. Elaborate final recommendation
	};
       
   \draw[myarrow] (defaf.south) -> (defparameters.north);
   \draw[myarrow] (defparameters.south) -> (initcentroids.north);
   \draw[myarrow] (initcentroids.south) -> (indices.north);
   \draw[myarrow] (indices.south) -> (assign.north);
   \draw[myarrow] (assign.south) -> (centroid.north);
  \draw[myarrow] (centroid.south) -> (decision.north);
  \draw[myarrow] (decision.south) node[left] {yes} -> (final.north);
  \draw[myarrow] (decision.west) |- node[above] {no} ++(-17mm,0) --  ($(indices.west) + (-15mm,0)$) -> (indices.west);

\end{tikzpicture}
}
\caption{A $K$-means and outranking-based approach to clustering}
\label{clustering}   
\end{center}
\end{figure}




%For a set $D=\{DM_1,DM_2,\ldots,DM_k\}$ of DMs, let $b \in B$ and $a \in A$ such that, for each $DM_k$ and $g_j$, the following fuzzy number is defined: 
%
%\begin{equation}
%\mu_j^k(a,b)=\min\{c_j^k(b,a), c^{inv}_j^k(a,b)\}.
%\label{trfnmu}
%\end{equation}
%
%\noindent
%that can be interpreted as a fuzzy indifference  relation, constructed from two outranking relations \citep{perny1992}. Thus, each $DM_k$ must provide information to elicit her/his own criteria weights $w_j^k (j=1,\ldots,m)$ (Step 2) and the hesitant outranking functions $\mu_j^k(a,b)$ (Step3). Next,  define the following HFEs 
%
%
%{\footnotesize
%\begin{eqnarray}
%hdir_j(b_h,a) 	&=&\{c_j^k(b_h,a) \mid k=1,\ldots,K\}, \label{hj}\\
%hinv_j(a,b_h)	&=&\{c^{inv}_j^k(a,b_h) \mid k=1,\ldots,K\}. \label{hinvj}
%\end{eqnarray}
%}
%
%\noindent
%Then, two levels of sorting are developed in this approach:
%
%\begin{enumerate}
%\item
%When the group level (all DMs) is considered, an HFS operator is proposed to  compute the credibility indices which are used in ELECTRE TRI-C to sort projects.  Thus, the HFS functions $sdir_j(b_h,a)$ and $sinv_j(a,b_h)$ are computed by aggregating  $hdir_j(b_h,a)$ and $hinv_j(a,b_h)$, respectively (Step 4.1). Next, for each $(a,b_h)$, the credibility that $b_h$ outranks $a$ and  the credibility that $a$ outranks $b_h$ are computed as follows (Step 4.2):
%
%\begin{eqnarray}
%\sigma_D(b_h,a) 	&=& \sum_{j=1}^{m} w_j sdir_j(b_h,a),\label{sigmaDG}\\
%\sigma_I(a,b_h) 	&=& \sum_{j=1}^{m} w_j sinv_j(a,b_h), \label{sigmaIG}
%\end{eqnarray} 
%
%
%\noindent
%where $w_j\geq0, \sum_{j=1}^{n} w_j=1$ is a set of weights that the set of DMs accept to correctly represent the importance of the criteria. \cite{peng2015} propose that a weight $w_j^k$  can be considered an HFE. Thus, a score function may be used to compute a criteria weight vector in the set $W_j=\{w_j^k \mid k=1,\ldots, K\}$, using the average score function proposed in \ref{hfsdef}.  Finally, the group credibility indices are used to apply the ELECTRE TRI-C assignment rules (Step 4.3). 
%
%\item
%When the individual level (each DM) is considered, the  respective elicited outranking functions  are used to sort projects. For each DM $k$ considered alone, her/his elicited criteria weights and outranking functions $\mu_j^k(a,b_h)$ are used to compute the individual credibility indices  as follows (Step 5.1),
%
%\begin{eqnarray}
%\sigma_D^k(b_h,a) 	&=& \sum_{j=1}^{m} w_j^k c_j^k(b_h,a),\label{sigmaDG}\\
%\sigma_I^k(a,b_h) 	&=& \sum_{j=1}^{m} w_j^k c^{inv}_j^k(a,b_h). \label{sigmaIG}
%\end{eqnarray} 
%
%\noindent
%Next, the ELECTRE TRI-C assignment rules are applied (Step 5.2). 
%
%\end{enumerate}


%Other approaches aggregating  \eqref{hj} and \eqref{hinvj}  have been used  to extend ELECTRE methods with HFS \citep{Jin2015,chen2015}.  
%In our case,  we consider two procedures  to aggregate HFEs (\cite{Farhadinia2014} provides other kinds of aggregation): an \emph{average} score function:
%
%\begin{equation}
%sdir_j(b_h,a) 	=\frac{1}{K}\sum_{k=1}^K c_j^k(b_h,a), \qquad sinv_j(a,b_h)=\frac{1}{K}\sum_{k=1}^K c^{inv}_j^k(b_h,a).\nonumber
%\end{equation}
% 
%\noindent
%and a \emph{max} score function:
%
%\begin{equation}
%sdir_j(b_h,a) 	=\max_{K} \{c_j^k(b_h,a)\}, \qquad sinv_j(a,b_h)	=\max_{K} \{c^{inv}_j^k(b_h,a)\}.\nonumber
%\end{equation}

In the following section, this approach is applied to a real problem.





\section{Application}\label{application}

\subsection{Problem}

%A Brazilian electrical power company has 49 projects that need to be clustered such that scarce and specialized resources could be allocated in a right manner. In a recent work by \cite{Oliveira2016},  a family of criteria was defined for this problem. The following criteria are defined in this problem: ($g_1$) \emph{Project Complexity}, which increases according to the increasing of budget and number of departments involved in the project's development; ($g_2$) \emph{Resources}, measuring the number of man-hours required to complete the project; ($g_3$)  \emph{Expected rate of development}, evaluating the urgency in project development and implementation; ($g_4$) \emph{Contribution} to the achievement of the organizational strategy; ($g_5$)  \emph{Technological level} involved in the project development. In Table \ref{criteria}, scales and weights, defined by the three DMs in the process, are shown.  
%
%
%\begin{table}[hbtp]
%\caption{Criteria, scales and weights}
%\label{criteria}
%\scriptsize
%\begin{center}
%\begin{tabular}{clllll}
%\hline
%						&\multicolumn{3}{c}{Weight}							&					&			\\\cline{2-4}
%Criterion					&DM1			&DM2			&DM3			&{\bf Verbal scale} 		&{\bf Scale} 	\\\hline
%\multirow{4}{*}{$g_1$}		& \multirow{4}{*}{0.20}& \multirow{4}{*}{0.15}& \multirow{4}{*}{0.30}&High				&4	 		\\
%						& 				&				&				&Medium				&3	 		\\
%						& 				&				&				&Low				&2	 		\\
%						& 				&				&				&Very low				&1	 		\\
%						& 				&				&				&					&	 		\\
%$g_2$					&0.20 				&0.15			&0.25			&					&Man-hours	 \\
%						& 				&				&				&					&	 		\\
%\multirow{5}{*}{$g_3$}		& \multirow{5}{*}{0.20}	& \multirow{5}{*}{0.20}	& \multirow{5}{*}{0.15}&Urgent				&5	 		\\
%						& 				&				&				&Critical				&4	 		\\
%						&				&				&				&Competitive			&3	 		\\
%						&				&				&				&Regular				&2	 		\\
%						&				&				&				&Low				&1	 		\\
%						&				&				&				&					&	 		\\
%\multirow{4}{*}{$g_4$}		& \multirow{4}{*}{0.30}& \multirow{4}{*}{0.15}& \multirow{4}{*}{0.15}	&Very high			&4	 		\\
%						& 				&				&				&High				&3	 		\\
%						& 				&				&				&Medium				&2	 		\\
%						& 				&				&				&Low				&1	 		\\
%						& 				&				&				&					&	 		\\
%\multirow{4}{*}{$g_5$}		& \multirow{4}{*}{0.10}& \multirow{4}{*}{0.35}& \multirow{4}{*}{0.15}	&High				&4	 		\\
%						& 				&				&				&Medium				&3	 		\\
%						& 				&				&				&Low				&2	 		\\
%						& 				&				&				&Negligible			&1	 		\\
%\hline
%
%\end{tabular}
%\end{center}
%\end{table}
%
%
%
%Evaluation of 49 projects are shown in Appendix  \ref{evaluations}. A senior manager helped to define reference alternatives, also shown in that table. According to  \cite{Oliveira2016}, three categories can be defined in this problem: $C_1$, the non-critical projects; $C_2$, the set of critical projects; $C_3$, the set of very critical projects.  


\section{Conclusions}\label{conclusions}
  


     
\section*{Acknowledgement}

%The authors are grateful for the support from FACEPE (PRONEX) and for the partial support of CNPq, Brazil.
          
\section*{References}
\bibliographystyle{authoryear}
\bibliography{../bib/mybib-mcdm,../bib/mybib-mcdm-v1,../bib/mybib}



%\newpage
%\appendices
%\section{Project evaluations}\label{evaluations}
%
%\begin{table}[hbtp]
%	\centering
%	\scriptsize
%	\begin{tabular}{rllrrrrr}
%Project	&	$g_1$	&	$g_2$	&	$g_3$	&	$g_4$	&	$g_5$	\\\hline
%1	&	2	&	640	&	2	&	3	&	2	\\
%2	&	3	&	480	&	2	&	2	&	1	\\
%3	&	3	&	640	&	1	&	3	&	2	\\
%4	&	1	&	720	&	2	&	2	&	2	\\
%5	&	1	&	160	&	3	&	3	&	2	\\
%6	&	2	&	160	&	2	&	3	&	2	\\
%7	&	3	&	620	&	2	&	1	&	2	\\
%8	&	2	&	640	&	1	&	2	&	1	\\
%9	&	1	&	320	&	2	&	3	&	3	\\
%10	&	2	&	320	&	2	&	3	&	2	\\
%11	&	2	&	160	&	3	&	3	&	1	\\
%12	&	1	&	160	&	1	&	1	&	2	\\
%13	&	2	&	640	&	3	&	3	&	2	\\
%14	&	2	&	240	&	3	&	3	&	2	\\
%15	&	1	&	160	&	2	&	2	&	1	\\
%16	&	1	&	160	&	2	&	3	&	1	\\
%17	&	3	&	320	&	3	&	2	&	1	\\
%18	&	3	&	640	&	2	&	3	&	1	\\
%19	&	3	&	960	&	2	&	3	&	2	\\
%20	&	1	&	160	&	2	&	2	&	2	\\
%21	&	2	&	640	&	2	&	2	&	2	\\
%22	&	1	&	160	&	2	&	2	&	3	\\
%23	&	3	&	1280	&	1	&	2	&	2	\\
%24	&	1	&	320	&	3	&	2	&	2	\\
%25	&	2	&	160	&	1	&	2	&	2	\\
%26	&	1	&	160	&	2	&	2	&	1	\\
%27	&	2	&	1280	&	1	&	3	&	2	\\
%28	&	3	&	640	&	2	&	2	&	2	\\
%29	&	2	&	800	&	1	&	2	&	2	\\
%30	&	2	&	160	&	3	&	2	&	2	\\
%31	&	2	&	160	&	1	&	2	&	1	\\
%32	&	3	&	320	&	1	&	2	&	2	\\
%33	&	2	&	240	&	2	&	3	&	1	\\
%34	&	2	&	320	&	2	&	2	&	1	\\
%35	&	1	&	120	&	2	&	3	&	1	\\
%36	&	2	&	160	&	1	&	3	&	1	\\
%37	&	2	&	3200	&	1	&	2	&	1	\\
%38	&	2	&	3200	&	1	&	2	&	1	\\
%39	&	2	&	960	&	1	&	2	&	1	\\
%40	&	2	&	3840	&	1	&	2	&	1	\\
%41	&	2	&	2880	&	1	&	2	&	1	\\
%42	&	1	&	1200	&	3	&	2	&	1	\\
%43	&	1	&	1200	&	3	&	2	&	1	\\
%44	&	3	&	7680	&	4	&	3	&	3	\\
%45	&	1	&	2400	&	3	&	2	&	1	\\
%46	&	3	&	5760	&	4	&	3	&	1	\\
%47	&	3	&	2880	&	4	&	3	&	1	\\
%48	&	2	&	7680	&	3	&	3	&	1	\\
%49	&	1	&	7680	&	3	&	3	&	1	\\\hline
%$b_1$	&	1	&	300	&	2	&	1	&	1	\\
%$b_2$	&	2	&	2000	&	3	&	2	&	2	\\
%$b_3$	&	3	&	5000	&	4	&	3	&	3	\\\hline
%\end{tabular}
%\end{table}
%

\end{document}

