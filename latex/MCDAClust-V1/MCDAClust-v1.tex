%\documentclass{iosart2c}
\documentclass[]{elsarticle}

\usepackage{natbib}
\usepackage{amsmath,amsthm,mathtools}
\usepackage{amssymb}
\usepackage{mathtools, cuted}

\usepackage[table, dvipsnames]{xcolor}
\usepackage{verbatim}

\usepackage{mathrsfs}
\usepackage{stfloats}
\usepackage{tabularx}
\usepackage{subfigure}

\usepackage{xtab,booktabs}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{tikz,ifthen,calc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.11}
\usepackage{tkz-fct}
\usepackage{color,colortbl}
\usepackage[labelfont=bf]{caption}

\usepackage{collcell}
\usepackage{hhline}
\usepackage{pgf}

\def\colorModel{hsb} %You can use rgb or hsb

\newcommand\ColCell[1]{
  \pgfmathparse{#1<1?1:0}  %Threshold for changing the font color into the cells
    \ifnum\pgfmathresult=0\relax\color{white}\fi
  \pgfmathsetmacro\compA{0}      %Component R or H
  \pgfmathsetmacro\compB{#1} %Component G or S
  \pgfmathsetmacro\compC{1}      %Component B or B
  \edef\x{\noexpand\centering\noexpand\cellcolor[\colorModel]{\compA,\compB,\compC}}\x #1
  } 
\newcolumntype{E}{>{\collectcell\ColCell}m{0.4cm}<{\endcollectcell}}  %Cell width


\usetikzlibrary{graphdrawing}
%\usetikzlibrary{graphicx}
%\usegdlibrary{trees}
\usetikzlibrary{shapes,decorations,arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows.meta}


\usepackage{enumitem}
\usepackage{algorithmic}
\usepackage{graphicx}
%\usepackage{subfig,graphicx}
\usepackage[titletoc,title]{appendix}

%\usepackage[blocks]{authblk}
%\newenvironment{keywords}{\noindent\textbf{Keywords:}}{}

\theoremstyle{definition}
\newtheorem*{inner}{\innerheader}
\newcommand{\innerheader}{}
\newenvironment{defi}[1]
 {\renewcommand\innerheader{#1}\begin{inner}}
 {\end{inner}}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\newtheorem{prop}{Theorem}


\newcolumntype{d}[1]{D{.}{.}{#1}}



\journal{}
\bibliographystyle{model2-names}\biboptions{authoryear}

\begin{document}
\begin{frontmatter}                           % The preamble begins here.


\title{Stochastic clustering in the Human Development Index problem based on the outranking preference relation and $K$-means}

%\runningtitle{Hesitant Fuzzy Sets and ELECTRE TRI-C}
%\subtitle{Subtitle}


%\author[A]{\fnms{Javier} \snm{Pereira}\thanks{Corresponding author. E-mail: jpereirar@inacap.cl.}},
%\author[B]{\fnms{Elaine C.B.} \snm{de Oliveira}}
%\author[C]{\fnms{Danielle C.} \snm{Morais}}
%\author[C]{\fnms{Ana Paula C.S.} \snm{Costa}}
%and
%\author[C]{\fnms{Luciana H.} \snm{Alencar}}
%
%\runningauthor{F. Author et al.}
%\address[A]{Universidad Tecnol\'ogica de Chile INACAP,  Santiago, Chile\\
%E-mail: xjavierpereira7@gmail.com}
%\address[B]{Instituto Federal da Paraiba, Campus Jo\~ao Pessoa, Jo\~ao Pessoa, Brazil\\
%E-mail: elainecjz@gmail.com}
%\address[C]{Universidade Federal de Pernambuco, Recife, Brazil\\
%E-mail: \{dcmorais,apcabral,lhalencar\}@cdsid.org.br}

\author[utc]{Javier Pereira$^{*,}$\cortext[cor1]}
\author[ap]{Pedro Contreras}
\author[ufpe]{Danielle C. Morais}
\author[itesm]{Pilar Arroyo-L\'opez}



\cortext[cor1]{Corresponding author at: xjavierpereira7@gmail.com (J.Pereira)}

\address[utc]{Universidad Tecnol\'ogica de Chile Inacap, Santiago, Chile (xjavierpereira7@gmail.com); \\}
\address[ap]{Berlin, Germany (pedro.contreras@gmail.com);}
\address[ufpe]{Universidade Federal de Pernambuco, CDSID (dcmorais@cdsid.org.br);}
\address[itesm]{Tecnologico de Monterrey, Campus Toluca, M\'exico (pilar.arroyo@itesm.mx);}



\begin{abstract}
The Human Development Index (HDI) has been proposed as a mean to encourage nations to focus on people capabilities to develop a country.  HDI calculation is based on three indices: the life expectancy, the years of education and the Gross National Income. Several studies criticize the highly compensatory nature of the computation method and seeming precision of the information sources. Last decade, non-compensatory clustering approaches that consider imprecise information, integrating the $K$-means algorithm,  have been proposed as an alternative to the ranking process underlying the HDI calculation. However, in these methods two main drawbacks can be highlighted. Firstly, uncertainty regarding parameters used to model imprecision is not considered. Secondly,  as a consequence, robust ordered clusters have not been considered yet. In this paper, a process integrating  the $K$-means algorithm is presented in which uncertain parameters are considered, also finding  a robust ordered clustering.  This approach is applied and results are discussed and compared with the 2018 HDI ranking.
\end{abstract}

\begin{keyword}
HDI \sep clustering \sep $K$-means \sep robustness \sep outranking relation 
\end{keyword}

\end{frontmatter}


\section{Introduction}

since 1990, the United Nations Development Programme (UNDP) releases the Human Development Index (HDI) to emphasize that people and their capabilities should be the ultimate criteria for assessing the development of a country, not only economic growth. The HDI ranking process evaluates  179 United Nations countries on the basis of three criteria: the life expectancy, the years of education and the Gross National Income (GNI). In addition, based on the HDI quartiles, four ordered groups of countries can be identified: very high human development, high human development, medium human development, and low human development.  

Despite the broad use of HDI  in business, policy-making, research, development-political debates, allocation of development aid, international climate accord designs, and economics \citep{Wolff2011}, it has been object of three main criticisms \citep{Noorbakhsh1998, Berenger2007, Klugman2011, Martinez2013}: (1) the compensatory effect; (2) the underlying concepts to define the dimensions; (3) the categorization process that uses the COP quartiles.  Regarding the compensatory effect, it is easy to see that a country exhibiting a bad performance in one index, can compensate it with a good performance in another. However, this is not desirable since, for instance, bad health indices can be hidden by good GNI, which can exacerbes inequality in access to health systems.   

Non-compensatory  methods have been proposed as an alternative to the HDI calculation method. For instance, \cite{Natoli2011} analyze three aggregation techniques designed for benchmarking and ranking of countries according to aggregated dimensions: additive methods, geometric aggregations, and non-compensatory methods.  They focus on the Condorcet approach as a non-compensatory aggregation technique for progress measures.  \cite{Lozano2009} propose to use the minimum of the component indexes in HDI instead of the arithmetic average. \cite{Mazziotta2015} compare two non-compensatory composite indices for measuring multidimensional phenomena and monitoring their changes over time: a non-linear composite index and a two-parameter function, which is an intermediate case between a compensatory and a full non-compensatory index. The authors found that the non-linear composite seems to be less compensatory that the second one.

Several scholars have proposed  ordered clustering strategies to partition the set of countries included in the HDI ranking,  based on non-compensatory multi-criteria approaches \citep{DeSmet2014}.   \cite{Boujelben2016} propose an approach that integrates PROMETHEE and $K$-means, a popular greedy algorithm for partitioning a set of elements into a pre-defined number of clusters so as to minimize the distances to the cluster centroids (the cluster centers). The underlying idea of their approach is based on the notion of belief distance between the alternatives and the centroids. \cite{Chen2018} propose a PROMETHEE-based algorithm, which also integrates the $K$-means rationale, to cluster countries in the HDI Report. They found that the ordered clustering is highly consistent with the HDI ranks. \cite{DeSmet2012} propose an exact multi-criteria algorithm to find HDI ordered categories, based on valued preference degrees.  They show the consistency between the obtained ordered partition and the HDI ranking.  \cite{Monteiro2018} propose a supervised classification approach based on the ELECTRE TRI method in which the HDI categories are defined \emph{a priori} by using fixed category boundaries. Different from other proposals using the outranking-based models, these authors consider that there is no imprecise data in HDI information sources. 

However, two types of imprecision have been highlighted by authors analyzing the quality of information used in the HDI calculation.  Firstly, the weighting process assigning equal importance levels to dimensions in HDI has been criticized and methods to deal with have been proposed \citep{Zheng2015}. In this article, we are not considering this kind of imprecision.  Secondly, the importance of considering imprecise data in constructing composite indicators has been remarked \citep{Cherchye2011}.  There are multiple indicators used for different kind of indexes, for instance, the Environmental Performance Index, the Internal Market Index, the Human Development Index, or the Technology Achievement Index. However, for some indicators only interval information is available, within what a true value is believed to lie.   \cite{Wolff2011} analyzed three sources of data error in HDI calculation and classification: measurement error due to data revisions, data error due to formula updating and misclassification due to inconsistent categories cut-off values. Actually,  measurement error from data updating impacts the rough data, the indices normalization and the HDI calculation. They found that from 11\% up to 34\% of all countries could be interpreted as misclassified in the development categories. These authors also found that on average the expected absolute deviation is nine rank positions. These results mean that interpretation of HDI ranks must be very careful.  Moreover, methods applied to HDI, which are based on the outranking relation, as for instance those using the PROMETHEE or the ELECTRE TRI methods, consider stable parameters.  However, this choice seems to be inadequate since the imprecision introduced by data updating and   inconsistent categories cut-off values.  Thus, parameter values could be supposed to be uncertain, something that has not been yet considered in the HDI clustering literature.

Essentially, clustering processes integrating $K$-means lie in two features \citep{DeSmet2009,Lolli2014,Panapakidis2018, Chen2018}. First, the definition of  an indifference-based metric that measures how indifferent is an alternative to a central alternative, or centroid, representing a category. Thus, an alternative is assigned to the cluster where the most indifferent centroid is found. Second, once all the alternatives have been assigned, the centroids are updated, using the information regarding the alternatives in their respective groups.  The process continues until a stop condition is satisfied. 

Although the $K$-means and the outranking-based methods can be used to find the same number of country clusters proposed by the ranking-based HDI methodology, these use compensatory net flow aggregation processes to obtain global preference relations. Thus, the methods do not necessarily guarantee the separability between any pair of central actions representing two different categories, a desired property of preference-based classification processes \citep{roy2012}. This means that a strict preference relation between any pair of centroids at each criterion level is not necessarily satisfied. As a consequence, supplementary procedures must be defined to order the clusters.  In these methods,  an alternative must be compared to each other, at least once. This information is stored in the form of a $n \times n$ matrix which must be traversed through on each iteration of the clustering algorithm. These are $O(IKn^2) \sim O(n^2)$ processes, where $I, K$ are the number of iterations of the algorithm (before finding a solution) and the number of clusters, respectively. We argue that a $O(Kn)$ method may be implemented for the assignment process by simply comparing each country at most to the $K$ centroids in the model.  Actually, we propose the usage of the ELECTRE TRI-C sorting method rationale to assign alternatives, in which central reference alternatives are used to define the categories, but updating these centroids by applying the $K$-means approach.

In this article, a stochastic approach for ordered clustering in the HDI problem is proposed, which integrates an outranking-based classification process with $K$-means, such that:  (1) building ordered clusters is an integral part of the procedure, without need for supplementary processes; an assignment rule proposed in the ELECTRE TRI-C sorting method \citep{almeida2008,Almeida2012} is used in the process, extending rules proposed in sorting methods to clustering; (2) uncertain thresholds are considered such that robustness analysis is performed. Thus, the proposed approach is integrated  to a Monte Carlo Simulation process in which sets of  parameter values are passed as input to the ordered clustering algorithm. To the best of our knowledge, this is the first approach that deals with these two problems.

The article is organized as follows......

%\section{Human Development Index}
%
%Since the first publication in 1990, the Human Development Index has been used as a benchmark to encourage development of nations and to foster policy makers to focus on three dimensions:  long and healthy life, access to knowledge and a decent standard of living.  The HDI is the geometric mean of normalized indices for each of the three dimensions, constructed from four indicators \citep{UNDP2019}: Life expectancy at birth (LE); Expected years of schooling (EYS); Mean years of schooling (MYS); and Gross National Income per capita (GNIpc) .  
%
%In order to calculate the HDI, two steps are applied: (1) Creating the dimension indices; (2) Aggregating the dimension indices into the HDI.  Step (1) consists of normalizing the four mentioned indicators according to the following formula:
%
%\begin{equation}
%\mbox{Dimension index}=\frac{\mbox{indicator value - minimum value}}{\mbox{maximum value - minimum value}}. \label{index}
%\end{equation}
%
%\noindent
%In the case of the two normalized education indices, the arithmetic mean is taken.  For the GNI, the natural logarithm of the indicator, the minimum and the maximum values is taken. Next, in Step (2), the geometric mean of indices is taken, as follows:
%
%\begin{equation}
%HDI= (I_{Health} \cdot I_{Education} \cdot I_{Income})^{1/3}. \label{HDI}
%\end{equation}
%
%\noindent
%It is worth to note that in this formula the same weights for all dimensions are given.   Based on the 2014 HDI, four categories of human development achievements are defined, using fixed cutoff points (COP) calculated using the quartiles ($q$) from the distributions of the component indicators averaged over 2004–2013:
%
%\begin{equation}
%COP_q= HDI(LE_q,EYS_q,MYS_q,GNIpc_q), \, q=1,2,3. \label{COP}
%\end{equation}
%
%\noindent
%These quartiles are used in the 2019's Report, as follows:
%
%
%\begin{table}[hbtp]
%\centering
%\begin{tabular}{ll}
%Category						&Limits \\
%\hline
%Very high human development 		& 0.800 and above \\
%\hline
%High human development			& 0.700-0.799		\\
%\hline
%Medium human development		& 0.550-0.699		\\
%\hline
%Low human development			& Below 0.550\\
%\hline
%\end{tabular}
%\end{table}
%
%
%%\subsection{HDI clustering problem}
%
%Three main criticisms to HDI have been reported by different authors \citep{Noorbakhsh1998, Berenger2007, Klugman2011, Martinez2013}: (1) the compensatory effect; (2) the underlying concepts to define the dimensions; (3) the categorization process that uses the COP quartiles.  Regarding the compensatory effect, it is easy to see that a country exhibiting a bad performance in one index, can compensate it with a good performance in another. However, this is not desirable since, for instance, bad health indices can be hidden by good GNI, which can exacerbes inequality in access to health systems.   
%
%Non-compensatory  methods have been proposed as an alternative to the HDI calculation method. For instance, \cite{Natoli2011} analyze three aggregation techniques designed for benchmarking and ranking of countries according to aggregated dimensions: additive methods, geometric aggregations, and non-compensatory methods.  They focus on the Condorcet approach as a non-compensatory aggregation technique for progress measures.  \cite{Lozano2009} propose to use the minimum of the component indexes in HDI instead of the arithmetic average. \cite{Mazziotta2015} compare two non-compensatory composite indices for measuring multidimensional phenomena and monitoring their changes over time: a non-linear composite index and a two-parameter function, which is an intermediate case between a compensatory and a full non-compensatory index. The authors found that the non-linear composite seems to be less compensatory that the second one.
%
%
%Non-compensatory  methods have been proposed to classify countries in the HDI Report.   \cite{Boujelben2016} propose an approach that integrates PROMETHEE and $K$-means. The underlying idea of their approach is based on the notion of belief distance between the alternatives and the centroids. \cite{Chen2018} propose a PROMETHEE-based algorithm, which integrates the $K$-means rationale, to cluster countries in the HDI Report. They found that the ordered clustering is highly consistent with the HDI ranks. \cite{DeSmet2012} propose an exact multi-criteria algorithm to find HDI ordered categories, based on valued preference degrees.  They show the consistency between the obtained ordered partition and the HDI ranking.  \cite{Monteiro2018} propose a supervised classification approach based on the ELECTRE TRI method in which the HDI categories are defined \emph{a priori} by using fixed category boundaries. Different from other proposals using the outranking-based models, these authors consider that $p=q=0$, that is, there is no imprecision. 
%
%Two types of imprecision have been highlighted by authors analyzing the quality of information used in the HDI calculation.  Firstly, the weighting process assigning equal importance levels to dimensions in HDI has been criticized and methods to deal with have been proposed \citep{Zheng2015}. In this article, we are not considering this kind of imprecision.  Secondly, the importance of considering imprecise data in constructing composite indicators has been remarked \citep{Cherchye2011}.  There are multiple indicators used for different kind of indexes, for instance, the Environmental Performance Index, the Internal Market Index, the Human Development Index, or the Technology Achievement Index. However, for some indicators only interval information is available, within what a true value is believed to lie.   \cite{Wolff2011} analyzed three sources of data error in HDI calculation and classification: measurement error due to data revisions, data error due to formula updating and misclassification due to inconsistent categories cut-off values. Actually,  measurement error from data updating impacts the rough data, the indices normalization and the HDI calculation. They found that from 11\% up to 34\% of all countries could be interpreted as misclassified in the development categories. These authors also found that on average the expected absolute deviation is nine rank positions. These results mean that interpretation of HDI ranks must be very careful.   
%
%
%
%Thus, several scholars have proposed  ordered clustering strategies to find a partition of countries, which we call the \emph{HDI problem}, based on non-compensatory multi-criteria approaches \citep{DeSmet2014,Boujelben2016}.  Most of non-compensatory approaches for computing the HDI clustering use the global outranking relation to model imprecise information regarding the evaluation of alternatives. Outranking was early proposed in Elimination Et Choix Traidusant la Realite (ELECTRE) methods \citep{figueira2010} and further became an essential feature in  Preference Ranking Organization METHod for Enrichment of Evaluations (PROMETHEE) methods \cite{brans85}. The outranking relation helps to model global  assertions from kind ``the alternative $b$ is at least as good as the alternative $a$'', based on information collected at the level of each criteria. 
%
%Some of the new MCDA clustering approaches to HDI integrate the $K$-means rationale \citep{DeSmet2009,Lolli2014,Panapakidis2018, Chen2018}, a popular greedy algorithm for partitioning a set of elements into a pre-defined number of clusters so as to minimize the distances to the cluster centroids (the cluster centers). Essentially, these clustering processes lie in two features of $K$-means. First, the definition of  an indifference-based metric measures how indifferent is an alternative to a centroid. Thus, an alternative is assigned to the cluster where the most indifferent centroid is found. Second, once all the alternatives have been assigned, the centroids are updated, using the information regarding the alternatives in their respective groups.  The process continues until a stop condition is satisfied. 
%
%Although the the $K$-means and outranking-based methods can be used to find the same number of country clusters proposed by the ranking-based HDI methodology, these use compensatory net flow aggregation processes to obtain global preference relations. Thus, the methods do not necessarily guarantee the separability between any pair of central actions representing two different categories, a desired property of preference-based classification processes \citep{roy2012}. This means that a strict preference relation between any pair of centroids at each criterion level is not necessarily satisfied. As a consequence, supplementary procedures must be defined to order the clusters.  In these methods,  an alternative must be compared to each other, at least once. This information is stored in the form of a $n \times n$ matrix which must be traversed through on each iteration of the clustering algorithm. These are $O(IKn^2) \sim O(n^2)$ processes, where $I, K$ are the number of iterations of the algorithm (before finding a solution) and the number of clusters, respectively. We argue that a $O(Kn)$ method may be implemented for the assignment process by simply comparing each country at most to the $K$ centroids in the model.  
%
%Methods based in the outranking relation need to model imprecise information by using two kind of parameters, defined for each criterion \citep{figueira2010}: the indifference and the preference thresholds. Most of time, these parameters are defined  with the decision-maker (DM) or expert judgement assistance. However, in the case of information regarding the HDI analysis, there is not an identified DM or even expert judgement can be variable because sources of information to evaluate countries in the three criteria is uncertain.  Thus, in this problem, considering single values does not seem appropriate.   
%
%
%In this article, we are going to model imprecision by the indifference and preference thresholds. However, differently from other outranking-based approaches, the parameter values are supposed to be realizations of  stochastic variables. Thus, the proposed approach is integrated  to a Monte Carlo Simulation process in which sets of  parameter values are passed as input to the ordered clustering algorithm.  In which follows, this procedure is explained in detail.



\section{Notation}\label{notation}

\subsection{Outranking relation}\label{classification}

Most of non-compensatory approaches for computing the HDI clustering use the global outranking relation to model imprecise information regarding the evaluation of alternatives. Outranking was early proposed in Elimination Et Choix Traidusant la Realite (ELECTRE) methods \citep{figueira2010} and further became an essential feature in  Preference Ranking Organization METHod for Enrichment of Evaluations (PROMETHEE) methods \cite{brans85}. The outranking relation helps to model global  assertions from kind ``the alternative $b$ is at least as good as the alternative $a$'', based on information collected at the level of each criteria. 

Non-compensatory methods based on the outranking relation could lead to higher computational complexity. However, problems exist in which compensation among criteria is not allowable. We place the present article in that kind of situations and propose a clustering model based on the outranking relation, as defined in the ELECTRE family of methods \citep{figueira2010}.

Let $A=\{a_1,a_2,\ldots,a_n\}$ be a set of actions (alternatives);  let $F=\{g_1,g_2,\ldots,g_m\}$ be a coherent family of criteria used to evaluate each action and $W=\{w_j \mid 0 \leq w_j \leq 1, \sum_{j=1}^m w_j =1\}$ the criteria weights.  The vector of performances of an action  $(g_1(a),g_2(a),\ldots,g_m(a))$   is called the \emph{evaluation profile}. 



The outranking relation, interpreted as the assertion ``$b$ is at least as good as $a$'',  can be measured on each criterion in terms of  the \emph{partial direct concordance index}, defined as follows: 

{\footnotesize
\begin{equation}
c_j(b,a) =
\begin{cases}
	0							& \mbox{if  $g_j(a)-g_j(b) > p_j$}, \\
	1							& \mbox{if  $g_j(a)-g_j(b) \leq q_j$},   \\
	\dfrac{g_j(b)-g_j(a)+p_j}{p_j-q_j} 	& \mbox{otherwise}, 				     	      
 \end{cases} 
 \label{credibility}
 \end{equation} 
}

\noindent
where $p_j,q_j$ are two imprecision parameters called direct preference threshold and  direct indifference threshold, respectively.  Similarly,  let us consider the assertion ``$a$ is at least as good as $b$'', by using inverse thresholds $p'_j,q'_j$, such that a \emph{partial inverse concordance index} may be defined as 

{\fontsize{7}{7}
\begin{equation}
c^{inv}_j(a,b) =
\begin{cases}
	0							& \mbox{if  $g_j(b)-g_j(a) > p'_j$}, \\
	1							& \mbox{if  $g_j(b)-g_j(a) \leq q'_j$},   \\
	\dfrac{g_j(a)-g_j(b)+p'_j}{p'_j-q'_j} 	& \mbox{otherwise}. 				     	      
 \end{cases} 
 \label{invcredibility}
 \end{equation} 
}


\noindent
A detailed explanation regarding the properties that direct and inverse thresholds must satisfy can be found in \cite{roy2012}.  The partial direct and inverse concordance indices allow to compute the global concordance indices as follows:

\begin{eqnarray}
\sigma_D(b_h,a) 	&=& \sum_{j=1}^{m} w_j c_j(b_h,a),\label{generalsigmaDG}\\
\sigma_I(a,b_h) 	&=& \sum_{j=1}^{m} w_j c^{inv}_j(a,b_h). \label{generalsigmaIG}
\end{eqnarray} 


\subsection{Outranking-based classification}\label{outrankingclass}

Let $C_h, (h=1,\ldots,H; H\geq 2)$ be a set of ordered categories such that $C_H \succ C_{H-1}\succ \ldots \succ C_1$, where  the relation $\succ$ means that for any $C_i, C_ j $ such that $i>j$, an element $a_i \in C_i$ is not worse than any element $a_j \in C_j$. Conversely, $a_j$ is worse than $a_i$, in terms of preferences.   A special set of actions $B=\{b_1,\ldots, b_H\}$ is defined, in which $b_h$ represents the \emph{centroid} of the category $C_h$. 



\def \q {1}
\def \p {2}
\def \qq {1.5}
\def \pp {3}
\def \xmina{-3}
\def \xmaxa{.5}
\def \xminb{-.5}
\def \xmaxb{4}
\def \ymax{1}
\def \xb{2*\p}

\begin{figure}[hbtp]
\scriptsize
\centering
\begin{tikzpicture}
\begin{axis}[
		clip=false,
		xmin=0,xmax=5,
		ymin=0,ymax=1,
		height=4cm,
		width=7cm,
		axis line style={draw=none}, 
		xticklabels=\empty,	
		xtick=\empty,
		xtick={0},	
		ytick=\empty,
		xtick pos=left,
		xtick distance=1,
		every axis x label/.style={at={(current axis.right of origin)},anchor=west}
		]
		\addplot [domain=-\p:-\q,smooth, thick] {(x+\p)/(\p-\q)};
		\addplot [domain=\xmina:-\p,smooth, thick] {0};
		\addplot [domain=-\q:\qq,smooth, thick] {1};
		\addplot [smooth,dashed]  coordinates {(-\q,0)(-\q,\ymax)} node{} ;
		\addplot [smooth,->]  coordinates {(\xmina,0)(\xmina,\ymax+.5)} node[above]{$\mu_j(a,b_h)$} ;
		\addplot [domain=\pp:\xmaxb,smooth, thick] {0};
		\addplot [domain=-\p:\xmaxb,smooth,thin,->] {0};
		\addplot [domain=\qq:\pp,smooth, thick] {(-x+\pp)/(\pp-\qq)};
		\addplot [smooth,dashed]  coordinates {(\qq,0)(\qq,\ymax)} node{} ;
    		\node  at (axis cs:\qq-.1,-0.1) {\tiny$10+q_j$};
    		\node  at (axis cs:\pp,-0.1) {\tiny$10+p_j$};
    		\node  at (axis cs:-\p-.3,-0.1) {\tiny$10-p_j'$};
    		\node  at (axis cs:-\q-.1,-0.1) {\tiny$10-q_j'$};
    		\node  at (axis cs:0,-0.1) {\tiny$10$};
		\node  at (axis cs:4.5,0) {${\tiny g_j(a)}$};
\end{axis}
\end{tikzpicture}
\caption{Membership function in continuous and discrete cases}
\label{mu}
\end{figure}


Let us define the following trapezoidal fuzzy number (TrFN)  \citep{Ban2011}:

\begin{equation}
\mu_j(a,b_h)=\min\{c_j(b_h,a), c^{inv}_j(a,b_h)\},
\label{trfnmu}
\end{equation}


\noindent
which can be interpreted as a fuzzy indifference  relation, constructed from two outranking relations \citep{perny1992}. In Figure \ref{mu},  an example is shown where $b_h$, with $g_j(b_h)=10$ on a criterion $j$,  is the centroid of a class $C_h$. Thus, any alternative $a$ can be evaluated as belonging to that class or not, by using the membership function $\mu_j(a,b_h)$.

Conversely, deconstructing $\mu_j(a,b_h)$ into the two outranking relations $c_j(b_h,a)$ and  $c^{inv}_j(a,b_h)$, the credibility indices \eqref{generalsigmaDG} and \eqref{generalsigmaIG} may be calculated and the following assignment rules can be defined \citep{pereira2018}:

\begin{enumerate}
\item
\emph{Descending Rule}. Let $\lambda \in [0.5,1]$ be a minimum credibility level. Decrease $h$ from $H+1$ until the first $t$ such that $\sigma_I(a,b_t)\geq \lambda$:

\begin{enumerate}
\item
If $t=H+1$, assign $a$ to $C_H$.
\item
If $t=0$, assign $a$ to $C_1$.
\item
For $0<t<H+1$, 

if $\min\{\sigma_I(a,b_t),\sigma_D(b_t,a)\} > \min\{\sigma_I(a,b_{t+1}),\sigma_D(b_{t+1},a)\}$ then assign $a$ to $C_t$; otherwise, assign $a$ to $C_{t+1}$.
\end{enumerate} 

\item
\emph{Ascending Rule}. Let $\lambda \in [0.5,1]$ be a minimum credibility level. Increase $h$ from $0$ until the first $t$ such that $\sigma_D(b_t,a)\geq \lambda$:

\begin{enumerate}
\item
If $t=1$, assign $a$ to $C_1$.
\item
If $t=H+1$, assign $a$ to $C_H$.
\item
For $0<t<H+1$, 

if $\min\{\sigma_I(a,b_t),\sigma_D(b_t,a)\} > \min\{\sigma_I(a,b_{t-1}),\sigma_D(b_{t-1},a)\}$ then assign $a$ to $C_t$; otherwise, assign $a$ to $C_{t-1}$.
\end{enumerate} 
\end{enumerate}


Essentially, the descending and the ascending rules evaluate how indifferent an alternative $a$ is to every $b_h$.  Thus, $a$ is assigned to the class for which it is closest.  In practice, these rules may assign an alternative to different classes. Therefore, in a particular application of the algorithm presented below, just one of them must be chosen.


\subsection{$K$-means algorithm}

$K$-means is a classical unsupervised method to partitioning a set of elements into a set of disjoint homogeneous groups, by minimizing the following objective function 

\begin{equation}
\min \sum_{h=1}^{K} \sum_{i=1}^{n} I_{ih} \left\| a_i-b_h \right\|,
\end{equation}


\noindent
where  $I_{ih}=1$ if $a_i \in C_h$, $0$ otherwise; and $ \left\| a_i-b_h \right\|$ is the Euclidean distance between $a_i$ and $b_h$.  Classical $K$-means algorithm consists of three steps: (1) define $A, K, b_h$ as inputs; (2) assign elements of $A$ into clusters; (3) update the cluster centers and return to step (2), until a stop condition is satisfied.   

Step (2) assigns an element $a_i$ to the group where the Euclidean distance between the element and the respective cluster center is minimized.  Step (3) updates the cluster centroids as follows


\begin{equation}
g_j(b_h) = \frac{1}{\mid C_h \mid} \sum_{a_i \in C_h} g_j(a_i) \quad j=1,\ldots,m. \label{newcentroid}
\end{equation}

\noindent
Thus, the algorithm is iterated a number of times, or until the  change of centroids is less or equal than a given tolerance.


\section{Clustering approach}\label{methodology}

The approach proposed here assumes that $K\geq 2$ ordered clusters must be found: $C_K \succ \ldots \succ C_1$. Similarly to the original $K$-means algorithm, an iterative constructive clustering process is performed until a stop condition is satisfied, as described  in Figure \ref{clustering}.  

Initially, the set of alternatives, the family of criteria, the criteria weights, and the number of clusters are identified.  Next, the imprecision parameters are set. Stochastic preference and indifference thresholds are considered in this approach. Thus, each time that the Step 2 is performed, a new set of values for $p_j,q_j,p'_j,q'_j \,(j=1,\ldots,m)$ are generated and used as input to the Step 3, where the new initial centroids are defined. Centroids  must satisfy the strict separability condition \citep{roy2012}:  for any pair $b_h^{(0)}, b_k^{(0)}$ such that $h>k$, it follows 

\begin{equation}
b_h^{(0)} \,P\, b_k^{(0)} \Leftrightarrow g_j(b_h^{(0)})-p'_j \geq g_j(b_k^{(0)})+p_j, \forall j.
\end{equation}

\noindent
where $P$ is a binary relation representing a strong preference.  This condition makes that fuzzy sets representing the membership functions of every category do not overlap. 


Step 4 consists of computing the credibility indices, using the expressions \eqref{generalsigmaDG} and  \eqref{generalsigmaIG}. In Step 5, the alternatives are assigned into the classes, using the descending (or the ascending) rule proposed in Section \ref{outrankingclass}.

In Step 6, the evaluation profile of each new centroid in the iteration $t$ and class $C_h$ is computed as follows:

\begin{equation}
g_j(b_h^{(t)}) = \frac{1}{\mid C_h^{t,j} \mid} \sum_{a_i \in C_h^{t,j}} g_j(a_i) \quad j=1,\ldots,m,\label{newcentroid}
\end{equation}

\noindent
{\color{red}where $C_h^{t,j}=\{a_i \in C_h \mid g_j(b_h^{(t)})-p'_j\leq g_j(a_i) \leq g_j(b_h^{(t)})+p_j\}$.  Whenever $C_h^{t,j}=\emptyset$, then $g_j(b_h^{(t)})=g_j(b_h^{(t-1)})$.   This condition helps to hold a weak separability condition, as shown below in the application. The weak separability condition says that given $h > t$, then $g_j(b_h) \geq g_j(b_t), \forall j$ \citep{roy2012}. This is an interesting property because it guarantees that the resulting clusters are ordered, as an outcome of the clustering approach.
}



\begin{figure}[hbtp]
\begin{center}
\fontsize{7}{7}{
\tikzstyle{bigbox}=[align=center,rectangle, draw=black, rounded corners,  anchor=north, minimum width=3.5cm, text width=3cm, minimum height=.7cm, node distance=.4cm]
\tikzstyle{myarrow}=[->, >= triangle 45]
\begin{tikzpicture}[
	scale=0.7,
	blueb/.style={
  		draw=black,
¡  		rounded corners,
  		text width=2cm,
  		font={\sffamily\color{black}}}
		]
    \node (defafw)[bigbox]
        {
            	1. Define $A, F, W, K, s=0$
        };
    \node (defpq)[bigbox,below= of defafw]
        {
            	2. Define $p_j,p'_j,q_j,q'_j,s=s+1$
        };
    \node (initcentroids)[bigbox,below= of defpq]
        {
            	3. Define $t=0, b_h^{(0)}$
        };
    \node (indices) [bigbox,   below=  of initcentroids]
        {
            	4. Compute $\sigma_D, \sigma_I$
	};
    \node (assign) [bigbox,   below=  of indices]
        {
            	5. Assign alternatives
	};
    \node (centroid) [bigbox,   below= of assign]
        {
            	6. Compute $t=t+1$ and  $b_h^{(t)}$ 
	};
   \node (decision) [draw, diamond, align=center, minimum width=2.5cm, minimum height=2.5cm, aspect=2, inner sep=-3pt, below=.4cm of centroid] 
        {
        $\mid b_h^{(t)} -b_h^{(t-1)} \mid < \epsilon$,\\
        or\\
	$t=Maxit$
        };
   
   \node (stochastic) [draw, diamond, align=center,  minimum width=3.0cm, minimum height=2.5cm, aspect=2, inner sep=-3pt, below=.4cm of decision] 
        {
	$s=Maxst$
        };

    \node (final) [bigbox,   below=of stochastic]
        {7. Compute frequency indices
	};
       
   \draw[myarrow] (defafw.south) -> (defpq.north);
   \draw[myarrow] (defpq.south) -> (initcentroids.north);
   \draw[myarrow] (initcentroids.south) -> (indices.north);
   \draw[myarrow] (indices.south) -> (assign.north);
   \draw[myarrow] (assign.south) -> (centroid.north);
  \draw[myarrow] (centroid.south) -> (decision.north);
  \draw[myarrow] (decision.south) node[left] {yes} -> (stochastic.north);
  \draw[myarrow] (decision.west) |- node[above] {no} ++(-17mm,0) --  ($(indices.west) + (-15mm,0)$) -> (indices.west);
  \draw[myarrow] (stochastic.south) node[left] {yes} -> (final.north);
  \draw[myarrow] (stochastic.west) |- node[above] {no} ++(-30mm,0) --  ($(defpq.west) + (-27mm,0)$) -> (defpq.west);

\end{tikzpicture}
}
\caption{Outranking-based approach to ordered clustering}
\label{clustering}   
\end{center}
\end{figure}

Given a set of parameter values, the inner loop in Figure \ref{clustering} iterates up to one out of two stop conditions applies: 1) the change of evaluation profiles is  lower than a pre-defined $\epsilon$ value, or 2) the maximum number of iterations, $Maxit$,  is reached.  The whole process is repeated $Maxst$ times, once for each set of stochastic parameter values. In Step 7, the final frequency indices are computed, proposing the $K$ ordered clusters.


  

In the following section, this approach is applied to the HDI problem.





\section{Clustering HDI countries}\label{application}

There are 189 countries to be clustered in the clustering process. The family of criteria to be considered consists of the three indices: life expectancy ($g_1$), years of schooling ($g_2$) and GNI  ($g_3$).  Data from the 2018's HDI Report is used to compute these indices, using the formulae defined in the HDI Technical Notes \citep{UNDP2019}. Four clusters, i.e. $K=4$, are defined, which parallels the number of groups defined in the Human Development Report.  As proposed by UNDP, weights are considered to be equal to $1/3$ for each criterion.

We assume that stochastic variables $\xi_{p_j},\xi_{q_j},\xi_{p'_j},\xi_{q'_j}$ correctly model each parameter uncertainty. In addition, it is assumed that each stochastic variable is uniformly distributed in an interval  $\xi \in [\xi_m, \xi_M]$. Thus, it is expected that the ''true'' value for each parameter lies within an interval, as follows:

\begin{eqnarray}
p_1,p'_1 \in [0.090, 0.290], \nonumber\\
p_2,p'_2 \in [0.040, 0.240], \nonumber\\
p_3,p'_3 \in [0.000, 0.200], \nonumber\\
q_1,q'_1 \in [0.045, 0.145], \nonumber\\
q_2,q'_2 \in [0.020, 0.120], \nonumber\\
q_3,q'_3 \in [0.000, 0.100]. \nonumber
\end{eqnarray}





\noindent
Thus, for each $p_j, q_j, p'_j, q'_j$, Monte Carlo Simulation allows to generate a set of 1000 parameter values. Consequently, $Maxst=1000$.  

\subsection{Results}

The proposed approach was implemented in Python, in Mac OSX. Time to execution was XYZ sec.  In Table \ref{results}, four groups of alternatives  are presented, the same defined in the HDI ranking.  Thus, for instance,  countries ranging from  1 (Norway) up to 62 (Seychelles) belong to the group of very high developed countries. Clusters built by the proposed approach are coded as $C_1 \prec  C_2 \prec C_3 \prec C_4$.  Given a row in a tabular structure, values indicate how frequently an alternative (country) was assigned to a cluster in the set fo 1000 simulations.  For instance, the alternative 1 in the group VERY HIGH DEVELOPMENT, 95\% of times was assigned into $C_4$, whilst 4\%, 1\%, and 0\% of times it was assigned to $C_3, C_2, C_1$, respectively.  Colors help to recognize these frequencies: the more intense a color, the higher the frequency an alternative reaches in a cluster.  As observed,  results generated with the stochastic approach show that almost all the alternatives assigned into $C_4$ have a frequency greater or equal to 53\%, except the alternative 60 (Bahamas) which appears belonging into that category only 44\% of times.  



\newcommand\items{4}   %Number of classes
\arrayrulecolor{white} %Table line colors
\begin{table}[hbtp]
\caption{Ordered clustering  vs HDI development groups}
\label{results}
\tiny
\hskip-4.0cm
\begin{tabular}{llll}
\begin{tabular}[t]{l*{\items}{|E}l}
\multicolumn{5}{c}{VERY HIGH DEVELOPMENT}\\\hline 
\multicolumn{1}{c}{Count} & 
\multicolumn{1}{c}{$C_1$} & 
\multicolumn{1}{c}{$C_2$} & 
\multicolumn{1}{c}{$C_3$} & 
\multicolumn{1}{c}{$C_4$} 
\\\hline
1	&	0.00 	&	0.01 	&	0.04 	&	0.95 	\\\hline
2	&	0.00 	&	0.00 	&	0.09 	&	0.91 	\\\hline
3	&	0.00 	&	0.01 	&	0.01 	&	0.98 	\\\hline
4	&	0.00 	&	0.00 	&	0.06 	&	0.94 	\\\hline
5	&	0.01 	&	0.01 	&	0.08 	&	0.89 	\\\hline
6	&	0.00 	&	0.01 	&	0.01 	&	0.98 	\\\hline
7	&	0.00 	&	0.01 	&	0.01 	&	0.98 	\\\hline
8	&	0.01 	&	0.01 	&	0.01 	&	0.98 	\\\hline
9	&	0.02 	&	0.02 	&	0.10 	&	0.87 	\\\hline
10	&	0.01 	&	0.01 	&	0.04 	&	0.95 	\\\hline
11	&	0.00 	&	0.01 	&	0.01 	&	0.98 	\\\hline
12	&	0.01 	&	0.01 	&	0.01 	&	0.98 	\\\hline
13	&	0.00 	&	0.01 	&	0.09 	&	0.90 	\\\hline
14	&	0.00 	&	0.01 	&	0.01 	&	0.98 	\\\hline
15	&	0.00 	&	0.01 	&	0.06 	&	0.92 	\\\hline
16	&	0.00 	&	0.01 	&	0.13 	&	0.86 	\\\hline
17	&	0.01 	&	0.01 	&	0.02 	&	0.97 	\\\hline
18	&	0.01 	&	0.04 	&	0.12 	&	0.82 	\\\hline
19	&	0.01 	&	0.03 	&	0.08 	&	0.88 	\\\hline
20	&	0.01 	&	0.01 	&	0.08 	&	0.90 	\\\hline
21	&	0.02 	&	0.06 	&	0.12 	&	0.80 	\\\hline
22	&	0.00 	&	0.02 	&	0.09 	&	0.90 	\\\hline
23	&	0.01 	&	0.01 	&	0.09 	&	0.89 	\\\hline
24	&	0.01 	&	0.01 	&	0.06 	&	0.92 	\\\hline
25	&	0.04 	&	0.03 	&	0.08 	&	0.86 	\\\hline
26	&	0.00 	&	0.02 	&	0.09 	&	0.88 	\\\hline
27	&	0.02 	&	0.04 	&	0.11 	&	0.84 	\\\hline
28	&	0.02 	&	0.02 	&	0.10 	&	0.85 	\\\hline
29	&	0.03 	&	0.03 	&	0.10 	&	0.84 	\\\hline
30	&	0.00 	&	0.02 	&	0.14 	&	0.84 	\\\hline
31	&	0.02 	&	0.04 	&	0.12 	&	0.82 	\\\hline
32	&	0.02 	&	0.03 	&	0.09 	&	0.86 	\\\hline
33	&	0.01 	&	0.02 	&	0.14 	&	0.84 	\\\hline
34	&	0.01 	&	0.03 	&	0.18 	&	0.78 	\\\hline
35	&	0.05 	&	0.11 	&	0.22 	&	0.62 	\\\hline
36	&	0.07 	&	0.12 	&	0.19 	&	0.62 	\\\hline
37	&	0.05 	&	0.09 	&	0.23 	&	0.63 	\\\hline
38	&	0.02 	&	0.06 	&	0.22 	&	0.69 	\\\hline
39	&	0.01 	&	0.04 	&	0.20 	&	0.74 	\\\hline
40	&	0.05 	&	0.04 	&	0.11 	&	0.80 	\\\hline
41	&	0.09 	&	0.14 	&	0.23 	&	0.54 	\\\hline
42	&	0.03 	&	0.03 	&	0.11 	&	0.83 	\\\hline
43	&	0.07 	&	0.11 	&	0.23 	&	0.58 	\\\hline
44	&	0.02 	&	0.06 	&	0.22 	&	0.70 	\\\hline
45	&	0.06 	&	0.09 	&	0.22 	&	0.63 	\\\hline
46	&	0.02 	&	0.06 	&	0.18 	&	0.74 	\\\hline
47	&	0.06 	&	0.10 	&	0.21 	&	0.64 	\\\hline
48	&	0.03 	&	0.06 	&	0.17 	&	0.74 	\\\hline
49	&	0.03 	&	0.10 	&	0.23 	&	0.65 	\\\hline
50	&	0.02 	&	0.06 	&	0.23 	&	0.68 	\\\hline
51	&	0.03 	&	0.09 	&	0.24 	&	0.64 	\\\hline
52	&	0.03 	&	0.08 	&	0.24 	&	0.66 	\\\hline
53	&	0.03 	&	0.07 	&	0.21 	&	0.69 	\\\hline
54	&	0.04 	&	0.09 	&	0.24 	&	0.63 	\\\hline
55	&	0.02 	&	0.07 	&	0.23 	&	0.68 	\\\hline
56	&	0.03 	&	0.07 	&	0.14 	&	0.75 	\\\hline
57	&	0.12 	&	0.15 	&	0.20 	&	0.53 	\\\hline
58	&	0.07 	&	0.08 	&	0.18 	&	0.67 	\\\hline
59	&	0.08 	&	0.10 	&	0.18 	&	0.63 	\\\hline
60	&	0.07 	&	0.15 	&	0.34 	&	0.44 	\\\hline
61	&	0.07 	&	0.13 	&	0.24 	&	0.56 	\\\hline
62	&	0.06 	&	0.11 	&	0.25 	&	0.57 	\\\hline
\end{tabular}

&

\begin{tabular}[t]{l*{\items}{|E}|}
\multicolumn{5}{c}{HIGH DEVELOPMENT}\\\hline 
\multicolumn{1}{c}{Count} & 
\multicolumn{1}{c}{$C_1$} & 
\multicolumn{1}{c}{$C_2$} & 
\multicolumn{1}{c}{$C_3$} & 
\multicolumn{1}{c}{$C_4$} 
\\\hline
63	&	0.03 	&	0.08 	&	0.22 	&	0.66 	\\\hline
64	&	0.07 	&	0.16 	&	0.33 	&	0.43 	\\\hline
65	&	0.05 	&	0.09 	&	0.23 	&	0.62 	\\\hline
66	&	0.06 	&	0.10 	&	0.24 	&	0.60 	\\\hline
67	&	0.08 	&	0.13 	&	0.23 	&	0.56 	\\\hline
68	&	0.06 	&	0.08 	&	0.14 	&	0.72 	\\\hline
69	&	0.04 	&	0.08 	&	0.18 	&	0.70 	\\\hline
70	&	0.02 	&	0.07 	&	0.24 	&	0.68 	\\\hline
71	&	0.05 	&	0.11 	&	0.23 	&	0.62 	\\\hline
72	&	0.03 	&	0.07 	&	0.17 	&	0.73 	\\\hline
73	&	0.10 	&	0.14 	&	0.25 	&	0.51 	\\\hline
74	&	0.09 	&	0.15 	&	0.24 	&	0.52 	\\\hline
75	&	0.07 	&	0.12 	&	0.22 	&	0.58 	\\\hline
76	&	0.09 	&	0.12 	&	0.23 	&	0.56 	\\\hline
77	&	0.09 	&	0.12 	&	0.20 	&	0.59 	\\\hline
78	&	0.08 	&	0.13 	&	0.24 	&	0.55 	\\\hline
79	&	0.08 	&	0.12 	&	0.21 	&	0.58 	\\\hline
80	&	0.08 	&	0.12 	&	0.20 	&	0.60 	\\\hline
81	&	0.05 	&	0.12 	&	0.30 	&	0.52 	\\\hline
82	&	0.09 	&	0.12 	&	0.20 	&	0.59 	\\\hline
83	&	0.07 	&	0.13 	&	0.24 	&	0.55 	\\\hline
84	&	0.08 	&	0.13 	&	0.22 	&	0.58 	\\\hline
85	&	0.10 	&	0.14 	&	0.20 	&	0.56 	\\\hline
86	&	0.06 	&	0.11 	&	0.22 	&	0.61 	\\\hline
87	&	0.09 	&	0.18 	&	0.32 	&	0.41 	\\\hline
88	&	0.05 	&	0.12 	&	0.24 	&	0.60 	\\\hline
89	&	0.10 	&	0.14 	&	0.23 	&	0.53 	\\\hline
90	&	0.09 	&	0.14 	&	0.21 	&	0.56 	\\\hline
91	&	0.10 	&	0.12 	&	0.20 	&	0.58 	\\\hline
92	&	0.10 	&	0.17 	&	0.24 	&	0.49 	\\\hline
93	&	0.15 	&	0.14 	&	0.22 	&	0.49 	\\\hline
94	&	0.14 	&	0.19 	&	0.31 	&	0.36 	\\\hline
95	&	0.11 	&	0.18 	&	0.27 	&	0.45 	\\\hline
96	&	0.08 	&	0.14 	&	0.30 	&	0.48 	\\\hline
97	&	0.10 	&	0.18 	&	0.32 	&	0.40 	\\\hline
98	&	0.12 	&	0.15 	&	0.20 	&	0.53 	\\\hline
99	&	0.13 	&	0.19 	&	0.25 	&	0.44 	\\\hline
100	&	0.12 	&	0.15 	&	0.29 	&	0.43 	\\\hline
101	&	0.12 	&	0.18 	&	0.30 	&	0.39 	\\\hline
102	&	0.10 	&	0.15 	&	0.30 	&	0.44 	\\\hline
103	&	0.08 	&	0.14 	&	0.30 	&	0.48 	\\\hline
104	&	0.20 	&	0.15 	&	0.18 	&	0.47 	\\\hline
105	&	0.07 	&	0.16 	&	0.24 	&	0.53 	\\\hline
106	&	0.12 	&	0.18 	&	0.31 	&	0.39 	\\\hline
107	&	0.12 	&	0.19 	&	0.31 	&	0.38 	\\\hline
108	&	0.24 	&	0.21 	&	0.32 	&	0.23 	\\\hline
109	&	0.10 	&	0.20 	&	0.31 	&	0.38 	\\\hline
110	&	0.15 	&	0.19 	&	0.28 	&	0.39 	\\\hline
111	&	0.15 	&	0.19 	&	0.28 	&	0.38 	\\\hline
112	&	0.08 	&	0.17 	&	0.33 	&	0.41 	\\\hline
113	&	0.19 	&	0.21 	&	0.26 	&	0.35 	\\\hline
114	&	0.10 	&	0.18 	&	0.23 	&	0.49 	\\\hline
115	&	0.21 	&	0.21 	&	0.27 	&	0.31 	\\\hline
116	&	0.16 	&	0.20 	&	0.25 	&	0.39 	\\\hline
\end{tabular}

&

\begin{tabular}[t]{l*{\items}{|E}|}
\multicolumn{5}{c}{MEDIUM DEVELOPMENT}\\\hline 
\multicolumn{1}{c}{Count} & 
\multicolumn{1}{c}{$C_1$} & 
\multicolumn{1}{c}{$C_2$} & 
\multicolumn{1}{c}{$C_3$} & 
\multicolumn{1}{c}{$C_4$} 
\\\hline
117	&	0.08 	&	0.16 	&	0.33 	&	0.44 	\\\hline
118	&	0.13 	&	0.15 	&	0.24 	&	0.49 	\\\hline
119	&	0.10 	&	0.15 	&	0.32 	&	0.43 	\\\hline
120	&	0.25 	&	0.19 	&	0.26 	&	0.30 	\\\hline
121	&	0.20 	&	0.15 	&	0.18 	&	0.48 	\\\hline
122	&	0.08 	&	0.18 	&	0.31 	&	0.43 	\\\hline
123	&	0.22 	&	0.19 	&	0.28 	&	0.31 	\\\hline
124	&	0.21 	&	0.18 	&	0.24 	&	0.36 	\\\hline
125	&	0.15 	&	0.19 	&	0.32 	&	0.34 	\\\hline
126	&	0.24 	&	0.19 	&	0.22 	&	0.35 	\\\hline
127	&	0.28 	&	0.18 	&	0.19 	&	0.35 	\\\hline
128	&	0.20 	&	0.17 	&	0.23 	&	0.40 	\\\hline
129	&	0.24 	&	0.21 	&	0.23 	&	0.32 	\\\hline
130	&	0.29 	&	0.21 	&	0.22 	&	0.27 	\\\hline
131	&	0.30 	&	0.18 	&	0.22 	&	0.30 	\\\hline
132	&	0.29 	&	0.19 	&	0.18 	&	0.35 	\\\hline
133	&	0.25 	&	0.20 	&	0.30 	&	0.25 	\\\hline
134	&	0.35 	&	0.15 	&	0.22 	&	0.28 	\\\hline
135	&	0.26 	&	0.19 	&	0.22 	&	0.33 	\\\hline
136	&	0.26 	&	0.20 	&	0.32 	&	0.22 	\\\hline
137	&	0.22 	&	0.21 	&	0.23 	&	0.34 	\\\hline
138	&	0.32 	&	0.20 	&	0.31 	&	0.16 	\\\hline
139	&	0.40 	&	0.19 	&	0.28 	&	0.13 	\\\hline
140	&	0.35 	&	0.18 	&	0.30 	&	0.18 	\\\hline
141	&	0.26 	&	0.19 	&	0.26 	&	0.29 	\\\hline
142	&	0.31 	&	0.21 	&	0.32 	&	0.16 	\\\hline
143	&	0.30 	&	0.21 	&	0.27 	&	0.22 	\\\hline
144	&	0.52 	&	0.20 	&	0.21 	&	0.07 	\\\hline
145	&	0.41 	&	0.19 	&	0.27 	&	0.13 	\\\hline
146	&	0.32 	&	0.18 	&	0.25 	&	0.25 	\\\hline
147	&	0.31 	&	0.20 	&	0.32 	&	0.16 	\\\hline
148	&	0.28 	&	0.19 	&	0.22 	&	0.31 	\\\hline
149	&	0.41 	&	0.17 	&	0.26 	&	0.16 	\\\hline
150	&	0.42 	&	0.17 	&	0.22 	&	0.19 	\\\hline
151	&	0.37 	&	0.23 	&	0.28 	&	0.12 	\\\hline
152	&	0.45 	&	0.18 	&	0.27 	&	0.10 	\\\hline
153	&	0.33 	&	0.18 	&	0.19 	&	0.30 	\\\hline
\end{tabular}

&

\begin{tabular}[t]{l*{\items}{|E}|}
\multicolumn{5}{c}{LOW DEVELOPMENT}\\\hline 
\multicolumn{1}{c}{Count} & 
\multicolumn{1}{c}{$C_1$} & 
\multicolumn{1}{c}{$C_2$} & 
\multicolumn{1}{c}{$C_3$} & 
\multicolumn{1}{c}{$C_4$} 
\\\hline
154	&	0.40 	&	0.16 	&	0.21 	&	0.23 	\\\hline
155	&	0.45 	&	0.19 	&	0.26 	&	0.10 	\\\hline
156	&	0.39 	&	0.18 	&	0.29 	&	0.14 	\\\hline
157	&	0.35 	&	0.17 	&	0.27 	&	0.21 	\\\hline
158	&	0.63 	&	0.16 	&	0.16 	&	0.05 	\\\hline
159	&	0.47 	&	0.20 	&	0.25 	&	0.08 	\\\hline
160	&	0.35 	&	0.19 	&	0.31 	&	0.15 	\\\hline
161	&	0.48 	&	0.18 	&	0.26 	&	0.08 	\\\hline
162	&	0.36 	&	0.21 	&	0.30 	&	0.13 	\\\hline
163	&	0.47 	&	0.14 	&	0.19 	&	0.19 	\\\hline
164	&	0.63 	&	0.15 	&	0.17 	&	0.05 	\\\hline
165	&	0.53 	&	0.20 	&	0.20 	&	0.06 	\\\hline
166	&	0.51 	&	0.17 	&	0.23 	&	0.09 	\\\hline
167	&	0.41 	&	0.17 	&	0.20 	&	0.21 	\\\hline
168	&	0.55 	&	0.17 	&	0.21 	&	0.07 	\\\hline
169	&	0.44 	&	0.21 	&	0.26 	&	0.09 	\\\hline
170	&	0.48 	&	0.18 	&	0.24 	&	0.10 	\\\hline
171	&	0.61 	&	0.16 	&	0.17 	&	0.05 	\\\hline
172	&	0.41 	&	0.17 	&	0.28 	&	0.14 	\\\hline
173	&	0.53 	&	0.18 	&	0.22 	&	0.07 	\\\hline
174	&	0.52 	&	0.19 	&	0.22 	&	0.08 	\\\hline
175	&	0.55 	&	0.20 	&	0.19 	&	0.06 	\\\hline
176	&	0.45 	&	0.19 	&	0.26 	&	0.09 	\\\hline
177	&	0.52 	&	0.18 	&	0.22 	&	0.08 	\\\hline
178	&	0.58 	&	0.16 	&	0.19 	&	0.07 	\\\hline
179	&	0.44 	&	0.23 	&	0.25 	&	0.08 	\\\hline
180	&	0.53 	&	0.20 	&	0.20 	&	0.07 	\\\hline
181	&	0.68 	&	0.13 	&	0.15 	&	0.04 	\\\hline
182	&	0.77 	&	0.10 	&	0.10 	&	0.03 	\\\hline
183	&	0.74 	&	0.09 	&	0.13 	&	0.03 	\\\hline
184	&	0.74 	&	0.17 	&	0.06 	&	0.03 	\\\hline
185	&	0.51 	&	0.15 	&	0.23 	&	0.12 	\\\hline
186	&	0.80 	&	0.14 	&	0.05 	&	0.01 	\\\hline
187	&	0.80 	&	0.14 	&	0.04 	&	0.01 	\\\hline
188	&	0.77 	&	0.17 	&	0.05 	&	0.02 	\\\hline
189	&	0.77 	&	0.14 	&	0.06 	&	0.03 	\\\hline
\end{tabular}

\end{tabular}

\end{table}


The group named HIGH DEVELOPMENT has an important number of countries that could be assigned into $C_4$, because of their high frequencies in that category.  In addition, note that frequencies in $C_3$ are less or equal to 33\%, which are weak arguments to state that these alternatives effectively belong to this group. However, it is easy to verify that, for each alternative, the accumulated frequency in $C_3, C_4$ show that arguments to be assigned into one of these two clusters is  high enough. 

The group named LOW DEVELOPMENT also reveals that most of the alternatives have accumulated frequencies in $C_1, C_2$ greater or equal to 50\%, which counts to assign them into one of these categories.  It is worth to note that the lower part of the tabular structure shows several countries which clearly belong to $C_1$.

The group named MEDIUM DEVELOPMENT shows that most alternatives are ambiguous in terms of assignment in $C_3,C_4$ or $C_1,C_2$. Actually, the upper part of this table seems to give more arguments in favor of alternatives to be allocated into $C_3,C_4$ (e.g., 117, 118, 119, 121), while the lower part is favorable to alternatives to be allocated into $C_1,C_2$ (e.g.,144, 149, 150, 152).

%Centroids found were as follows
%
%\begin{eqnarray}
%b_1 = [0.3, 0.0, 0.5], \nonumber\\
%b_2 = [0.5, 0.3, 0.7], \nonumber\\
%b_3 = [0.6, 0.4, 0.8], \nonumber\\
%b_4 = [0.8, 0.5,0.9]. \nonumber
%\end{eqnarray}
%
% 
%\noindent
%Note that centroids satisfy the weak separability condition, i.e. given $h > t$, then $g_j(b_h) \geq g_j(b_t), \forall j$.  Thus, the final clustering is ordered in the sense 



\subsection{Discussion}




\section{Conclusions}\label{conclusions}
  


     
\section*{Acknowledgement}

          
\section*{References}
\bibliographystyle{authoryear}
\bibliography{../bib/mybib-mcdm,../bib/mybib-mcdm-v1,../bib/mybib}





\appendix
\section{Separability condition}\label{sepcondition}


\end{document}

