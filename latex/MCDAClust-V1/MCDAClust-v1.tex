%\documentclass{iosart2c}
\documentclass[]{elsarticle}

\usepackage{natbib}
\usepackage{amsmath,amsthm,mathtools}
\usepackage{amssymb}
\usepackage{mathtools, cuted}

\usepackage[table, dvipsnames]{xcolor}
\usepackage{verbatim}

\usepackage{mathrsfs}
\usepackage{stfloats}
\usepackage{tabularx}
\usepackage{xtab,booktabs}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{tikz,ifthen,calc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.11}
\usepackage{tkz-fct}
\usepackage{color,colortbl}
\usepackage[labelfont=bf]{caption}


\usetikzlibrary{graphdrawing}
%\usetikzlibrary{graphicx}
%\usegdlibrary{trees}
\usetikzlibrary{shapes,decorations,arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows.meta}


\usepackage{enumitem}
\usepackage{algorithmic}
\usepackage{subcaption,graphicx}
%\usepackage{subfig,graphicx}
\usepackage[titletoc,title]{appendix}

%\usepackage[blocks]{authblk}
%\newenvironment{keywords}{\noindent\textbf{Keywords:}}{}

\theoremstyle{definition}
\newtheorem*{inner}{\innerheader}
\newcommand{\innerheader}{}
\newenvironment{defi}[1]
 {\renewcommand\innerheader{#1}\begin{inner}}
 {\end{inner}}

\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\newtheorem{prop}{Theorem}


\newcolumntype{d}[1]{D{.}{.}{#1}}



\journal{}
\bibliographystyle{model2-names}\biboptions{authoryear}

\begin{document}
\begin{frontmatter}                           % The preamble begins here.


\title{Clustering in the Human Development Index problem based on the outranking preference relation and $K$-means}

%\runningtitle{Hesitant Fuzzy Sets and ELECTRE TRI-C}
%\subtitle{Subtitle}


%\author[A]{\fnms{Javier} \snm{Pereira}\thanks{Corresponding author. E-mail: jpereirar@inacap.cl.}},
%\author[B]{\fnms{Elaine C.B.} \snm{de Oliveira}}
%\author[C]{\fnms{Danielle C.} \snm{Morais}}
%\author[C]{\fnms{Ana Paula C.S.} \snm{Costa}}
%and
%\author[C]{\fnms{Luciana H.} \snm{Alencar}}
%
%\runningauthor{F. Author et al.}
%\address[A]{Universidad Tecnol\'ogica de Chile INACAP,  Santiago, Chile\\
%E-mail: xjavierpereira7@gmail.com}
%\address[B]{Instituto Federal da Paraiba, Campus Jo\~ao Pessoa, Jo\~ao Pessoa, Brazil\\
%E-mail: elainecjz@gmail.com}
%\address[C]{Universidade Federal de Pernambuco, Recife, Brazil\\
%E-mail: \{dcmorais,apcabral,lhalencar\}@cdsid.org.br}

\author[utc]{Javier Pereira$^{*,}$\cortext[cor1]}
\author[ap]{Pedro Contreras}
\author[ufpe]{Danielle C. Morais}
\author[itesm]{Pilar Arroyo-L\'opez}



\cortext[cor1]{Corresponding author at: xjavierpereira7@gmail.com (J.Pereira)}

\address[utc]{Universidad Tecnol\'ogica de Chile Inacap, Santiago, Chile (xjavierpereira7@gmail.com); \\}
\address[ap]{Berlin, Germany (pedro.contreras@gmail.com);}
\address[ufpe]{Universidade Federal de Pernambuco, CDSID (dcmorais@cdsid.org.br);}
\address[itesm]{Tecnologico de Monterrey, Campus Toluca, M\'exico (pilar.arroyo@itesm.mx);}



\begin{abstract}
The human development index (HDI) has been proposed as a mean to foster nations to put focus on people capabilities to develop a country.  HDI calculation bases on three criteria: the life expectancy, the years of education and the Gross National Income. Several studies criticize the highly compensatory nature and apparently precision of the computation method. Last past decade, non-compensatory clustering approaches that consider imprecise information and integrate the $K$-means algorithm  have been proposed as an alternative to the ranking process underlying the HDI computation. However, in these methods two main drawbacks can be mentioned. Firstly, uncertainty regarding parameters used to model imprecision is not considered. Secondly, separability of cluster centers is not guaranteed and complementary procedures are needed to find an ordered clustering. In this paper, a process integrating  the $K$-means algorithm is presented in which uncertain parameters are considered, also satisfying a separability condition between pairs of centroids. Thus,  a robust ordered clustering  is obtained.   Results are compared with outcomes obtained with PROMETHEE-based and other outranking-based approaches.
\end{abstract}

\begin{keyword}
HDI index \sep clustering \sep $K$-means \sep robustness \sep outranking relation 
\end{keyword}

\end{frontmatter}


\section{Introduction}

The human development index (HDI) was created by the United Nations Development Programme (UNDP) to emphasize that people and their capabilities should be the ultimate criteria for assessing the development of a country, not only economic growth. The United Nations Development Program (UNDP) has proposed the HDI ranking which is used to evaluate  179 United Nations countries on the basis of three criteria: the life expectancy, the years of education and the Gross National Income (GNI). For each country, a ranking can be obtained by aggregating these three criteria.  HDI is computed as the geometric mean of normalized indices for each one of these criteria.  In 2018, the Human Development Report Office, responsible for the HDI, identified four ordered groups of countries, based on quartiles: very high human development, high human development, medium human development, and low human development.   

HDI has been object of criticism because of the highly compensatory computation model and the lack of consideration in which regards the precision of information available to evaluate the countries and the information available to model preferences \citep{Noorbakhsh1998, Berenger2007, Klugman2011, Martinez2013}.  Thus, several scholars have proposed  ordered clustering strategies to find a partition of countries, which we call the \emph{HDI problem}, based on non-compensatory multi-criteria approaches \citep{DeSmet2014,Boujelben2016}.  Most of non-compensatory approaches for computing the HDI clustering use the global outranking relation to model imprecise information regarding the evaluation of alternatives. Outranking was early proposed in Elimination Et Choix Traidusant la Realite (ELECTRE) methods \citep{figueira2010} and further became an essential feature in  Preference Ranking Organization METHod for Enrichment of Evaluations (PROMETHEE) methods \cite{brans85}. The outranking relation helps to model global  assertions from kind ``the alternative $b$ is at least as good as the alternative $a$'', based on information collected at the level of each criteria. 

Some of the new MCDA clustering approaches to HDI integrate the $K$-means rationale \citep{DeSmet2009,Lolli2014,Panapakidis2018, Chen2018}, a popular greedy algorithm for partitioning a set of elements into a pre-defined number of clusters so as to minimize the distances to the cluster centroids (the cluster centers). Essentially, these clustering processes lie in two features of $K$-means. First, the definition of  an indifference-based metric measures how indifferent is an alternative to a centroid. Thus, an alternative is assigned to the cluster where the most indifferent centroid is found. Second, once all the alternatives have been assigned, the centroids are updated, using the information regarding the alternatives in their respective groups.  The process continues until a stop condition is satisfied. 

Although the the $K$-means and outranking-based methods can be used to find the same number of country clusters proposed by the ranking-based HDI methodology, these use compensatory net flow aggregation processes to obtain global preference relations. Thus, the methods do not necessarily guarantee the separability between any pair of central actions representing two different categories, a desired property of preference-based classification processes \citep{roy2012}. This means that a strict preference relation between any pair of centroids at each criterion level is not necessarily satisfied. As a consequence, supplementary procedures must be defined to order the clusters.  In these methods,  an alternative must be compared to each other, at least once. This information is stored in the form of a $n \times n$ matrix which must be traversed through on each iteration of the clustering algorithm. These are $O(IKn^2) \sim O(n^2)$ processes, where $I, K$ are the number of iterations of the algorithm (before finding a solution) and the number of clusters, respectively. We argue that a $O(Kn)$ method may be implemented for the assignment process by simply comparing each country at most to the $K$ centroids in the model.  

Methods based in the outranking relation need to model imprecise information by using two kind of parameters, defined for each criterion \citep{figueira2010}: the indifference and the preference thresholds. Most of time, these parameters are defined  with the decision-maker (DM) or expert judgement assistance. However, in the case of information regarding the HDI analysis, there is not an identified DM or even expert judgement can be variable because sources of information to evaluate countries in the three criteria is uncertain.  Thus, in this problem, considering single values does not seem appropriate.   

In this article, we take into account these two difficulties and propose a new approach for ordered clustering in the HDI problem which integrates an outranking-based classification process with $K$-means such that:  (1) building ordered clusters is an integral part of the procedure without need for supplementary processes; an assignment rule proposed in the ELECTRE TRI-C sorting method \citep{almeida2008,Almeida2012} is used in the process, extending rules proposed in sorting methods to clustering; (2) uncertain thresholds are considered such that robustness analysis is performed. To the best of our knowledge, this is the first approach that deals with these two problems.  The main contribution of this paper consists of an stochastic approach to HDI ordered clustering.

The article is organized as follows......

%\section{Clustering in MCDA}
%
%Most problems in data analysis regard the partition of a set of elements, often described by multiple dimensions, into a set of homogeneous groups. There are two elementary ways to group elements: supervised and unsupervised methods.  The first concerns methods in which the elements are assigned into pre-defined categories. In such cases, the problem consists of identifying the ``right'' category for each element. The unsupervised methods are used in cases in which  a best partitioning of the set of elements must be discovered, such that the some distance metric is minimized between a pair of elements in a group and maximized between a pair of elements in different groups.  Clustering has become an important issue in Multi-Criteria Decision Analysis (MCDA).  Most decision-making problems require the assignment of alternatives into groups such that differentiated treatments and strategies can be defined, which implementation depends on the characteristics exhibited by alternatives belonging to each group. 
%
%Last past decades,  several authors have proposed new MCDA methods that  integrate  $K$-means. However, two main issues must be considered. First, in data analysis,  a distance metric is defined to compare any pair of elements. In MCDA, instead, preference relations must be used to compare pairs of alternatives, which is often modeled by using value functions or outranking relations \citep{roy96, figueira2010}. Thus, the main difference between the classification strategies in data analysis  and the MCDA classification processes lies in the way that dimensions used to represent the elements are conceived.  In data analysis,  dimensions are conceptualized as attributes that help to characterize the elements in order to explore or recognize data patterns. Thus, two elements belonging to the same group are said \emph{mutually similar}.  In MCDA,  dimensions are used to evaluate elements and any pair of elements can be compared in terms of preference relations by using these evaluations. Although some authors have proposed similarity measures to determine whether an alternative belong to a group or not, it has been proposed that is better to use indifference measures  \citep{Olteanu2013}. Thus, two elements belonging to the same group are said \emph{mutually indifferent}. Nevertheless, below we propose that reviewing the indifference relation between the alternatives and the centroids also allows to discover ordered groups.
%
%Secondly, in MCDA two unsupervised strategies are recognized, the first one called the \emph{ordered clustering}, in which clusters are completely ordered, and  \emph{relational clustering} which purpose is finding more complex order relations between clusters \citep{zopounidis2002, DeSmet2009}. $K$-means implements an unsupervised strategy to discover un-ordered clusters. Thus, integrating this algorithm into an MCDA method supposes the definition of a strategy to order the clusters built by the algorithm process.  Compensatory methods have incorporated these conditions.  A method is compensatory whether bad performance on some criteria can be offset by good performance on others.  Instead, non-compensatory methods use parameters and implement aggregation rules that allow to eliminate alternatives that perform very bad in one or several criteria \citep{figueira2010}.   \cite{Lolli2014} integrate $K$-means with the AHP method and apply it to a multi-criteria inventory classification problem. \cite{Panapakidis2018} integrate TOPSIS and $K$-means to cluster user load profiles based on consumption data (for instance, in energy related fields), weather data, consumer preferences, occupancy behavior and others. This is also a compensatory strategy.
%
%For which concerns non-compensatory approaches, \cite{Chen2018} propose a PROMETHEE II and $K$-means algorithm which considers the preference degree between any two alternatives. A relative net flow metric, based on the outranking relation of PROMETHEE, is proposed to measure the closeness of alternatives and centroids. An ordered clustering is found by defining a \emph{preference distance} metric based on the global net flow of any alternative in a cluster.  \cite{Fernandez2010} also propose a model based on the outranking relation to cluster alternatives, using a $K$-means strategy. They define an indifference outranking relation to evaluate closeness of alternatives and centroids. Given that this clustering process does not necessarily produces an ordered set of clusters, the authors propose a specific method to build that clustering structure.  \cite{DeSmet2012} address the problem of regrouping alternatives into completely ordered categories based on valued preference degrees. An exact algorithm is proposed to find the ordered partition. They propose a model that takes as input both the number of clusters, fixed \emph{a priori}, and a valued preference matrix which stores the global strict preference value between any pair of alternatives. To compute the strict preference value, an outranking-based function is formulated. The purpose in that article is to find an ordered partition so as to minimize the inconsistencies with the information contained in this matrix. The inconsistencies regard the wrong assignment of pairs of alternatives into clusters  such that this does not satisfy the alternatives preference structure stored in the matrix. 
%
%
%First, it can be verified that, given two centroids from successive ordered clusters, these methods do not necessarily satisfy  a consistency property which requires a strict preference relation be satisfied between any pair of centroids at each criterion level.  This is a structural feature in the ELECTRE TRI sorting methods \citep{roy2012}, which guarantees the separability between any pair of central actions representing two different categories. We argue that paying attention to this property, a clustering process can be defined, by integrating a $K$-means-like mechanism to update the CRA as if these were centroids, but in a way that the clusters order is preserved through the clustering process. 
%
%Second, the assignment based in the indifference relation considers that any  alternative in a cluster should not be preferable to another inside the same group.   However, by construction, the alternatives in the cluster should be indifferent to the current centroid and not necessarily indifferent each other.  In which follows, we propose an algorithm based on this idea.  We show that this is an efficient way to obtain ordered clusters.


\section{Notation}\label{notation}

\subsection{Outranking relation}\label{classification}

Non-compensatory methods based on the outranking relation could lead to higher computational complexity. However, problems exist in which compensation among criteria is not allowable. We place the present article in that kind of situations and propose a clustering model based on the outranking relation, as defined in the ELECTRE family of methods \citep{figueira2010}.

Let $A=\{a_1,a_2,\ldots,a_n\}$ be a set of actions (alternatives);  let $F=\{g_1,g_2,\ldots,g_m\}$ be a coherent family of criteria used to evaluate each action and $W=\{w_j \mid 0 \leq w_j \leq 1, \sum_{j=1}^m w_j =1\}$ the criteria weights.  The vector of performances of an action  $(g_1(a),g_2(a),\ldots,g_m(a))$   is called the \emph{evaluation profile}. 



The outranking relation, interpreted as the assertion ``$b$ is at least as good as $a$'',  can be measured on each criterion in terms of  the \emph{partial direct concordance index}, defined as follows: 

{\footnotesize
\begin{equation}
c_j(b,a) =
\begin{cases}
	0							& \mbox{if  $g_j(a)-g_j(b) > p_j$}, \\
	1							& \mbox{if  $g_j(a)-g_j(b) \leq q_j$},   \\
	\dfrac{g_j(b)-g_j(a)+p_j}{p_j-q_j} 	& \mbox{otherwise}, 				     	      
 \end{cases} 
 \label{credibility}
 \end{equation} 
}

\noindent
where $p_j,q_j$ are two imprecision parameters called direct preference threshold and  direct indifference threshold, respectively.  Similarly,  let us consider the assertion ``$a$ is at least as good as $b$'', by using inverse thresholds $p'_j,q'_j$, such that a \emph{partial inverse concordance index} may be defined as 

{\fontsize{7}{7}
\begin{equation}
c^{inv}_j(a,b) =
\begin{cases}
	0							& \mbox{if  $g_j(b)-g_j(a) > p'_j$}, \\
	1							& \mbox{if  $g_j(b)-g_j(a) \leq q'_j$},   \\
	\dfrac{g_j(a)-g_j(b)+p'_j}{p'_j-q'_j} 	& \mbox{otherwise}. 				     	      
 \end{cases} 
 \label{invcredibility}
 \end{equation} 
}


\noindent
A detailed explanation regarding the properties that direct and inverse thresholds must satisfy can be found in \cite{roy2012}.  The partial direct and inverse concordance indices allow to compute the global concordance indices as follows:

\begin{eqnarray}
\sigma_D(b_h,a) 	&=& \sum_{j=1}^{m} w_j c_j(b_h,a),\label{generalsigmaDG}\\
\sigma_I(a,b_h) 	&=& \sum_{j=1}^{m} w_j c^{inv}_j(a,b_h). \label{generalsigmaIG}
\end{eqnarray} 


\subsection{Outranking-based classification}\label{outrankingclass}

Let $C_h, (h=1,\ldots,H; H\geq 2)$ be a set of ordered categories such that $C_H \succ C_{H-1}\succ \ldots \succ C_1$, where  the relation $\succ$ means that for any $C_i, C_ j $ such that $i>j$, an element $a_i \in C_i$ is not worse than any element $a_j \in C_j$. Conversely, $a_j$ is worse than $a_i$, in terms of preferences.   A special set of actions $B=\{b_1,\ldots, b_H\}$ is defined, in which $b_h$ represents the \emph{centroid} of the category $C_h$. 



\def \q {1}
\def \p {2}
\def \qq {1.5}
\def \pp {3}
\def \xmina{-3}
\def \xmaxa{.5}
\def \xminb{-.5}
\def \xmaxb{4}
\def \ymax{1}
\def \xb{2*\p}

\begin{figure}[hbtp]
\scriptsize
\centering
\begin{tikzpicture}
\begin{axis}[
		clip=false,
		xmin=0,xmax=5,
		ymin=0,ymax=1,
		height=4cm,
		width=7cm,
		axis line style={draw=none}, 
		xticklabels=\empty,	
		xtick=\empty,
		xtick={0},	
		ytick=\empty,
		xtick pos=left,
		xtick distance=1,
		every axis x label/.style={at={(current axis.right of origin)},anchor=west}
		]
		\addplot [domain=-\p:-\q,smooth, thick] {(x+\p)/(\p-\q)};
		\addplot [domain=\xmina:-\p,smooth, thick] {0};
		\addplot [domain=-\q:\qq,smooth, thick] {1};
		\addplot [smooth,dashed]  coordinates {(-\q,0)(-\q,\ymax)} node{} ;
		\addplot [smooth,->]  coordinates {(\xmina,0)(\xmina,\ymax+.5)} node[above]{$\mu_j(a,b_h)$} ;
		\addplot [domain=\pp:\xmaxb,smooth, thick] {0};
		\addplot [domain=-\p:\xmaxb,smooth,thin,->] {0};
		\addplot [domain=\qq:\pp,smooth, thick] {(-x+\pp)/(\pp-\qq)};
		\addplot [smooth,dashed]  coordinates {(\qq,0)(\qq,\ymax)} node{} ;
    		\node  at (axis cs:\qq-.1,-0.1) {\tiny$10+q_j$};
    		\node  at (axis cs:\pp,-0.1) {\tiny$10+p_j$};
    		\node  at (axis cs:-\p-.3,-0.1) {\tiny$10-p_j'$};
    		\node  at (axis cs:-\q-.1,-0.1) {\tiny$10-q_j'$};
    		\node  at (axis cs:0,-0.1) {\tiny$10$};
		\node  at (axis cs:4.5,0) {${\tiny g_j(a)}$};
\end{axis}
\end{tikzpicture}
\caption{Membership function in continuous and discrete cases}
\label{mu}
\end{figure}


Let us define the following trapezoidal fuzzy number (TrFN)  \citep{Ban2011}:

\begin{equation}
\mu_j(a,b_h)=\min\{c_j(b_h,a), c^{inv}_j(a,b_h)\},
\label{trfnmu}
\end{equation}


\noindent
which can be interpreted as a fuzzy indifference  relation, constructed from two outranking relations \citep{perny1992}. In Figure \ref{mu},  an example is shown where $b_h$, with $g_j(b_h)=10$ on a criterion $j$,  is the centroid of a class $C_h$. Thus, any alternative $a$ can be evaluated as belonging to that class or not, by using the membership function $\mu_j(a,b_h)$.

Conversely, deconstructing $\mu_j(a,b_h)$ into the two outranking relations $c_j(b_h,a)$ and  $c^{inv}_j(a,b_h)$, the credibility indices \eqref{generalsigmaDG} and \eqref{generalsigmaIG} may be calculated and the following assignment rules can be defined \citep{pereira2018}:

\begin{enumerate}
\item
\emph{Descending Rule}. Let $\lambda \in [0.5,1]$ be a minimum credibility level. Decrease $h$ from $H+1$ until the first $t$ such that $\sigma_I(a,b_t)\geq \lambda$:

\begin{enumerate}
\item
If $t=H+1$, assign $a$ to $C_H$.
\item
If $t=0$, assign $a$ to $C_1$.
\item
For $0<t<H+1$, 

if $\min\{\sigma_I(a,b_t),\sigma_D(b_t,a)\} > \min\{\sigma_I(a,b_{t+1}),\sigma_D(b_{t+1},a)\}$ then assign $a$ to $C_t$; otherwise, assign $a$ to $C_{t+1}$.
\end{enumerate} 

\item
\emph{Ascending Rule}. Let $\lambda \in [0.5,1]$ be a minimum credibility level. Increase $h$ from $0$ until the first $t$ such that $\sigma_D(b_t,a)\geq \lambda$:

\begin{enumerate}
\item
If $t=1$, assign $a$ to $C_1$.
\item
If $t=H+1$, assign $a$ to $C_H$.
\item
For $0<t<H+1$, 

if $\min\{\sigma_I(a,b_t),\sigma_D(b_t,a)\} > \min\{\sigma_I(a,b_{t-1}),\sigma_D(b_{t-1},a)\}$ then assign $a$ to $C_t$; otherwise, assign $a$ to $C_{t-1}$.
\end{enumerate} 
\end{enumerate}


Essentially, the descending and the ascending rules evaluate how indifferent an alternative $a$ is to every $b_h$.  Thus, $a$ is assigned to the class for which it is closest.  In practice, these rules may assign an alternative to different classes. Therefore, in a particular application of the algorithm presented below, just one of them must be chosen.


\subsection{$K$-means algorithm}

$K$-means is a classical unsupervised method to partitioning a set of elements into a set of disjoint homogeneous groups, by minimizing the following objective function 

\begin{equation}
\min \sum_{h=1}^{K} \sum_{i=1}^{n} I_{ih} \left\| a_i-b_h \right\|,
\end{equation}


\noindent
where  $I_{ih}=1$ if $a_i \in C_h$, $0$ otherwise; and $ \left\| a_i-b_h \right\|$ is the Euclidean distance between $a_i$ and $b_h$.  Classical $K$-means algorithm consists of three steps: (1) define $A, K, b_h$ as inputs; (2) assign elements of $A$ into clusters; (3) update the cluster centers and return to step (2), until a stop condition is satisfied.   

Step (2) assigns an element $a_i$ to the group where the Euclidean distance between the element and the respective cluster center is minimized.  Step (3) updates the cluster centroids as follows


\begin{equation}
g_j(b_h) = \frac{1}{\mid C_h \mid} \sum_{a_i \in C_h} g_j(a_i) \quad j=1,\ldots,m. \label{newcentroid}
\end{equation}

\noindent
Thus, the algorithm is iterated a number of times, or until the  change of centroids is less or equal than a given tolerance.


\section{Clustering approach}\label{methodology}

The approach proposed here assumes that $K\geq 2$ ordered clusters must be found: $C_K \succ \ldots \succ C_1$. Similarly to the original $K$-means algorithm, iterations are performed until a stop condition is satisfied, as described  in Figure \ref{clustering}.  

Initially, the set of alternatives, the family of criteria, the criteria weights, and the number of clusters are identified.  Next, the imprecision parameters are set. In this step, stochastic parameter values are generated.  The clustering algorithm is initialized in step 3, defining the initial centroids which must satisfy the strict separability condition \citep{roy2012}:  for any pair $b_h^{(0)}, b_k^{(0)}$ such that $h>k$, it follows 

\begin{equation}
b_h^{(0)} \,P\, b_k^{(0)} \Leftrightarrow g_j(b_h^{(0)})-p'_j \geq g_j(b_k^{(0)})+p_j, \forall j.
\end{equation}

\noindent
where $P$ is a binary relation representing a strong preference.  This condition makes that fuzzy sets representing the membership functions of every category do not overlap. 


Step 4 consists of computing the credibility indices, using the expressions \eqref{generalsigmaDG} and  \eqref{generalsigmaIG}. In step 5, the alternatives are assigned into the classes, using the descending (or the ascending) rule proposed in Section \ref{outrankingclass}.

In Step 6, the evaluation profile of each new centroid in the iteration $t$ and class $C_h$ is computed as follows:

\begin{equation}
g_j(b_h^{(t)}) = \frac{1}{\mid C_h^{int,j} \mid} \sum_{a_i \in C_h^{int,j}} g_j(a_i) \quad j=1,\ldots,m,\label{newcentroid}
\end{equation}

\noindent
where $C_h^{int,j}=\{a_i \in C_h \mid g_j(b_h^{(t)})-p'_j\leq g_j(a_i) \leq g_j(b_h^{(t)})+p_j\}$.  Whenever $C_h^{int,j}=\emptyset$, then $g_j(b_h^{(t)})=g_j(b_h^{(t-1)})$.   This condition holds the separability condition, as shown in the following theorem.

\begin{prop}
If  $g_j(b_h^{(t)})$ is defined by using \eqref{newcentroid}, then $b_h^{(t)} \,P\, b_k^{(t)}, h>k$.
\end{prop}

\noindent
The proof of this theorem is provided in the \ref{sepcondition}.

The algorithm iterates up to one out of two stop conditions applies: 1) the change of evaluation profiles is  lower than a pre-defined $\epsilon$ value, or 2) the maximum number of iterations, $Maxit$,  is reached.  The algorithm is repeated $Maxst$ times, once for each set of stochastic parameter values. In step 7, the final frequency indices are computed, proposing the $K$ ordered clusters.



\begin{figure}[hbtp]
\begin{center}
\fontsize{7}{7}{
\tikzstyle{bigbox}=[align=center,rectangle, draw=black, rounded corners,  anchor=north, minimum width=3.5cm, text width=3cm, minimum height=.7cm, node distance=.4cm]
\tikzstyle{myarrow}=[->, >= triangle 45]
\begin{tikzpicture}[
	scale=0.7,
	blueb/.style={
  		draw=black,
ยก  		rounded corners,
  		text width=2cm,
  		font={\sffamily\color{black}}}
		]
    \node (defafw)[bigbox]
        {
            	1. Define $A, F, W, K$
        };
    \node (defpq)[bigbox,below= of defafw]
        {
            	2. Define $p_j,p'_j,q_j,q'_j$
        };
    \node (initcentroids)[bigbox,below= of defpq]
        {
            	3. Define $t=0, b_h^{(0)}$
        };
    \node (indices) [bigbox,   below=  of initcentroids]
        {
            	4. Compute credibility indices $\sigma_D, \sigma_I$
	};
    \node (assign) [bigbox,   below=  of indices]
        {
            	5. Assign alternatives
	};
    \node (centroid) [bigbox,   below= of assign]
        {
            	6. Compute new \\
	 centroids $b_h^{(t)}$ and $t=t+1$ 
	};
   \node (decision) [draw, diamond, align=center, minimum width=2.5cm, minimum height=2.5cm, aspect=2, inner sep=-3pt, below=.4cm of centroid] 
        {
        $\mid b_h^{(t)} -b_h^{(t-1)} \mid < \epsilon$,\\
        or\\
	$t=Maxit$
        };
   
   \node (stochastic) [draw, diamond, align=center,  minimum width=3.0cm, minimum height=2.5cm, aspect=2, inner sep=-3pt, below=.4cm of decision] 
        {
	$s=Maxst$
        };

    \node (final) [bigbox,   below=of stochastic]
        {7. Compute frequency indices
	};
       
   \draw[myarrow] (defafw.south) -> (defpq.north);
   \draw[myarrow] (defpq.south) -> (initcentroids.north);
   \draw[myarrow] (initcentroids.south) -> (indices.north);
   \draw[myarrow] (indices.south) -> (assign.north);
   \draw[myarrow] (assign.south) -> (centroid.north);
  \draw[myarrow] (centroid.south) -> (decision.north);
  \draw[myarrow] (decision.south) node[left] {yes} -> (stochastic.north);
  \draw[myarrow] (decision.west) |- node[above] {no} ++(-17mm,0) --  ($(indices.west) + (-15mm,0)$) -> (indices.west);
  \draw[myarrow] (stochastic.south) node[left] {yes} -> (final.north);
  \draw[myarrow] (stochastic.west) |- node[above] {no} ++(-30mm,0) --  ($(defpq.west) + (-27mm,0)$) -> (defpq.west);

\end{tikzpicture}
}
\caption{Outranking-based approach to ordered clustering}
\label{clustering}   
\end{center}
\end{figure}




%For a set $D=\{DM_1,DM_2,\ldots,DM_k\}$ of DMs, let $b \in B$ and $a \in A$ such that, for each $DM_k$ and $g_j$, the following fuzzy number is defined: 
%
%\begin{equation}
%\mu_j^k(a,b)=\min\{c_j^k(b,a), c^{inv}_j^k(a,b)\}.
%\label{trfnmu}
%\end{equation}
%
%\noindent
%that can be interpreted as a fuzzy indifference  relation, constructed from two outranking relations \citep{perny1992}. Thus, each $DM_k$ must provide information to elicit her/his own criteria weights $w_j^k (j=1,\ldots,m)$ (Step 2) and the hesitant outranking functions $\mu_j^k(a,b)$ (Step3). Next,  define the following HFEs 
%
%
%{\footnotesize
%\begin{eqnarray}
%hdir_j(b_h,a) 	&=&\{c_j^k(b_h,a) \mid k=1,\ldots,K\}, \label{hj}\\
%hinv_j(a,b_h)	&=&\{c^{inv}_j^k(a,b_h) \mid k=1,\ldots,K\}. \label{hinvj}
%\end{eqnarray}
%}
%
%\noindent
%Then, two levels of sorting are developed in this approach:
%
%\begin{enumerate}
%\item
%When the group level (all DMs) is considered, an HFS operator is proposed to  compute the credibility indices which are used in ELECTRE TRI-C to sort projects.  Thus, the HFS functions $sdir_j(b_h,a)$ and $sinv_j(a,b_h)$ are computed by aggregating  $hdir_j(b_h,a)$ and $hinv_j(a,b_h)$, respectively (Step 4.1). Next, for each $(a,b_h)$, the credibility that $b_h$ outranks $a$ and  the credibility that $a$ outranks $b_h$ are computed as follows (Step 4.2):
%
%\begin{eqnarray}
%\sigma_D(b_h,a) 	&=& \sum_{j=1}^{m} w_j sdir_j(b_h,a),\label{sigmaDG}\\
%\sigma_I(a,b_h) 	&=& \sum_{j=1}^{m} w_j sinv_j(a,b_h), \label{sigmaIG}
%\end{eqnarray} 
%
%
%\noindent
%where $w_j\geq0, \sum_{j=1}^{n} w_j=1$ is a set of weights that the set of DMs accept to correctly represent the importance of the criteria. \cite{peng2015} propose that a weight $w_j^k$  can be considered an HFE. Thus, a score function may be used to compute a criteria weight vector in the set $W_j=\{w_j^k \mid k=1,\ldots, K\}$, using the average score function proposed in \ref{hfsdef}.  Finally, the group credibility indices are used to apply the ELECTRE TRI-C assignment rules (Step 4.3). 
%
%\item
%When the individual level (each DM) is considered, the  respective elicited outranking functions  are used to sort projects. For each DM $k$ considered alone, her/his elicited criteria weights and outranking functions $\mu_j^k(a,b_h)$ are used to compute the individual credibility indices  as follows (Step 5.1),
%
%\begin{eqnarray}
%\sigma_D^k(b_h,a) 	&=& \sum_{j=1}^{m} w_j^k c_j^k(b_h,a),\label{sigmaDG}\\
%\sigma_I^k(a,b_h) 	&=& \sum_{j=1}^{m} w_j^k c^{inv}_j^k(a,b_h). \label{sigmaIG}
%\end{eqnarray} 
%
%\noindent
%Next, the ELECTRE TRI-C assignment rules are applied (Step 5.2). 
%
%\end{enumerate}


%Other approaches aggregating  \eqref{hj} and \eqref{hinvj}  have been used  to extend ELECTRE methods with HFS \citep{Jin2015,chen2015}.  
%In our case,  we consider two procedures  to aggregate HFEs (\cite{Farhadinia2014} provides other kinds of aggregation): an \emph{average} score function:
%
%\begin{equation}
%sdir_j(b_h,a) 	=\frac{1}{K}\sum_{k=1}^K c_j^k(b_h,a), \qquad sinv_j(a,b_h)=\frac{1}{K}\sum_{k=1}^K c^{inv}_j^k(b_h,a).\nonumber
%\end{equation}
% 
%\noindent
%and a \emph{max} score function:
%
%\begin{equation}
%sdir_j(b_h,a) 	=\max_{K} \{c_j^k(b_h,a)\}, \qquad sinv_j(a,b_h)	=\max_{K} \{c^{inv}_j^k(b_h,a)\}.\nonumber
%\end{equation}

In the following section, this approach is applied to a real problem.





\section{Application}\label{application}

\subsection{Problem}

%A Brazilian electrical power company has 49 projects that need to be clustered such that scarce and specialized resources could be allocated in a right manner. In a recent work by \cite{Oliveira2016},  a family of criteria was defined for this problem. The following criteria are defined in this problem: ($g_1$) \emph{Project Complexity}, which increases according to the increasing of budget and number of departments involved in the project's development; ($g_2$) \emph{Resources}, measuring the number of man-hours required to complete the project; ($g_3$)  \emph{Expected rate of development}, evaluating the urgency in project development and implementation; ($g_4$) \emph{Contribution} to the achievement of the organizational strategy; ($g_5$)  \emph{Technological level} involved in the project development. In Table \ref{criteria}, scales and weights, defined by the three DMs in the process, are shown.  
%
%
%\begin{table}[hbtp]
%\caption{Criteria, scales and weights}
%\label{criteria}
%\scriptsize
%\begin{center}
%\begin{tabular}{clllll}
%\hline
%						&\multicolumn{3}{c}{Weight}							&					&			\\\cline{2-4}
%Criterion					&DM1			&DM2			&DM3			&{\bf Verbal scale} 		&{\bf Scale} 	\\\hline
%\multirow{4}{*}{$g_1$}		& \multirow{4}{*}{0.20}& \multirow{4}{*}{0.15}& \multirow{4}{*}{0.30}&High				&4	 		\\
%						& 				&				&				&Medium				&3	 		\\
%						& 				&				&				&Low				&2	 		\\
%						& 				&				&				&Very low				&1	 		\\
%						& 				&				&				&					&	 		\\
%$g_2$					&0.20 				&0.15			&0.25			&					&Man-hours	 \\
%						& 				&				&				&					&	 		\\
%\multirow{5}{*}{$g_3$}		& \multirow{5}{*}{0.20}	& \multirow{5}{*}{0.20}	& \multirow{5}{*}{0.15}&Urgent				&5	 		\\
%						& 				&				&				&Critical				&4	 		\\
%						&				&				&				&Competitive			&3	 		\\
%						&				&				&				&Regular				&2	 		\\
%						&				&				&				&Low				&1	 		\\
%						&				&				&				&					&	 		\\
%\multirow{4}{*}{$g_4$}		& \multirow{4}{*}{0.30}& \multirow{4}{*}{0.15}& \multirow{4}{*}{0.15}	&Very high			&4	 		\\
%						& 				&				&				&High				&3	 		\\
%						& 				&				&				&Medium				&2	 		\\
%						& 				&				&				&Low				&1	 		\\
%						& 				&				&				&					&	 		\\
%\multirow{4}{*}{$g_5$}		& \multirow{4}{*}{0.10}& \multirow{4}{*}{0.35}& \multirow{4}{*}{0.15}	&High				&4	 		\\
%						& 				&				&				&Medium				&3	 		\\
%						& 				&				&				&Low				&2	 		\\
%						& 				&				&				&Negligible			&1	 		\\
%\hline
%
%\end{tabular}
%\end{center}
%\end{table}
%
%
%
%Evaluation of 49 projects are shown in Appendix  \ref{evaluations}. A senior manager helped to define reference alternatives, also shown in that table. According to  \cite{Oliveira2016}, three categories can be defined in this problem: $C_1$, the non-critical projects; $C_2$, the set of critical projects; $C_3$, the set of very critical projects.  


\section{Conclusions}\label{conclusions}
  


     
\section*{Acknowledgement}

%The authors are grateful for the support from FACEPE (PRONEX) and for the partial support of CNPq, Brazil.
          
\section*{References}
\bibliographystyle{authoryear}
\bibliography{../bib/mybib-mcdm,../bib/mybib-mcdm-v1,../bib/mybib}



%\newpage
%\appendices
%\section{Project evaluations}\label{evaluations}
%
%\begin{table}[hbtp]
%	\centering
%	\scriptsize
%	\begin{tabular}{rllrrrrr}
%Project	&	$g_1$	&	$g_2$	&	$g_3$	&	$g_4$	&	$g_5$	\\\hline
%1	&	2	&	640	&	2	&	3	&	2	\\
%2	&	3	&	480	&	2	&	2	&	1	\\
%3	&	3	&	640	&	1	&	3	&	2	\\
%4	&	1	&	720	&	2	&	2	&	2	\\
%5	&	1	&	160	&	3	&	3	&	2	\\
%6	&	2	&	160	&	2	&	3	&	2	\\
%7	&	3	&	620	&	2	&	1	&	2	\\
%8	&	2	&	640	&	1	&	2	&	1	\\
%9	&	1	&	320	&	2	&	3	&	3	\\
%10	&	2	&	320	&	2	&	3	&	2	\\
%11	&	2	&	160	&	3	&	3	&	1	\\
%12	&	1	&	160	&	1	&	1	&	2	\\
%13	&	2	&	640	&	3	&	3	&	2	\\
%14	&	2	&	240	&	3	&	3	&	2	\\
%15	&	1	&	160	&	2	&	2	&	1	\\
%16	&	1	&	160	&	2	&	3	&	1	\\
%17	&	3	&	320	&	3	&	2	&	1	\\
%18	&	3	&	640	&	2	&	3	&	1	\\
%19	&	3	&	960	&	2	&	3	&	2	\\
%20	&	1	&	160	&	2	&	2	&	2	\\
%21	&	2	&	640	&	2	&	2	&	2	\\
%22	&	1	&	160	&	2	&	2	&	3	\\
%23	&	3	&	1280	&	1	&	2	&	2	\\
%24	&	1	&	320	&	3	&	2	&	2	\\
%25	&	2	&	160	&	1	&	2	&	2	\\
%26	&	1	&	160	&	2	&	2	&	1	\\
%27	&	2	&	1280	&	1	&	3	&	2	\\
%28	&	3	&	640	&	2	&	2	&	2	\\
%29	&	2	&	800	&	1	&	2	&	2	\\
%30	&	2	&	160	&	3	&	2	&	2	\\
%31	&	2	&	160	&	1	&	2	&	1	\\
%32	&	3	&	320	&	1	&	2	&	2	\\
%33	&	2	&	240	&	2	&	3	&	1	\\
%34	&	2	&	320	&	2	&	2	&	1	\\
%35	&	1	&	120	&	2	&	3	&	1	\\
%36	&	2	&	160	&	1	&	3	&	1	\\
%37	&	2	&	3200	&	1	&	2	&	1	\\
%38	&	2	&	3200	&	1	&	2	&	1	\\
%39	&	2	&	960	&	1	&	2	&	1	\\
%40	&	2	&	3840	&	1	&	2	&	1	\\
%41	&	2	&	2880	&	1	&	2	&	1	\\
%42	&	1	&	1200	&	3	&	2	&	1	\\
%43	&	1	&	1200	&	3	&	2	&	1	\\
%44	&	3	&	7680	&	4	&	3	&	3	\\
%45	&	1	&	2400	&	3	&	2	&	1	\\
%46	&	3	&	5760	&	4	&	3	&	1	\\
%47	&	3	&	2880	&	4	&	3	&	1	\\
%48	&	2	&	7680	&	3	&	3	&	1	\\
%49	&	1	&	7680	&	3	&	3	&	1	\\\hline
%$b_1$	&	1	&	300	&	2	&	1	&	1	\\
%$b_2$	&	2	&	2000	&	3	&	2	&	2	\\
%$b_3$	&	3	&	5000	&	4	&	3	&	3	\\\hline
%\end{tabular}
%\end{table}
%


\appendix
\section{Separability condition}\label{sepcondition}


\end{document}

